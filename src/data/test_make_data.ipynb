{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad7b1192-e47e-4954-95f0-2e7285675203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from src.data.dataset_femto import create_femto_dataset\n",
    "from src.data.data_utils import (\n",
    "    get_min_max,\n",
    "    scaler,\n",
    "    create_x_y,\n",
    "    create_date_dict,\n",
    ")\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from src.features.build_features import build_spectrogram_df_femto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c594555-cf11-4ad5-876d-8052f2a0d588",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path.cwd().parent.parent\n",
    "input_filepath = 'data/raw'\n",
    "output_filepath = 'data/processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ed9f0bf-a129-4149-b473-f59c86dbfe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_size=64\n",
    "random_state=694\n",
    "\n",
    "\n",
    "# input file paths for FEMTO (PRONOSTIA) and IMS data sets\n",
    "folder_raw_data_train = (\n",
    "    root_dir / input_filepath / \"FEMTO/Learning_set/\"\n",
    ") \n",
    "\n",
    "folder_raw_data_test = (\n",
    "    root_dir / input_filepath / \"FEMTO/Test_set/\"\n",
    ")\n",
    "\n",
    "folder_raw_data_ims = (\n",
    "    root_dir / input_filepath / \"IMS/\"\n",
    ")\n",
    "\n",
    "# processed data file paths\n",
    "folder_processed_data_femto = (root_dir / output_filepath / 'FEMTO/')\n",
    "folder_processed_data_ims = (root_dir / output_filepath / 'IMS/')\n",
    "\n",
    "# make processed directories if not exist\n",
    "folder_processed_data_femto.mkdir(parents=True, exist_ok=True)\n",
    "folder_processed_data_ims.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# create_femto_dataset(folder_raw_data_train_femto, folder_raw_data_test_femto, \n",
    "# folder_processed_data_femto, bucket_size=64, random_state=694)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ec47a56-138b-4b98-9f26-cf14acb8132e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (2800, 20)\n",
      "x_train.shape: (3709, 20)\n",
      "x_train.shape: (4222, 20)\n",
      "x_val.shape: (869, 20)\n",
      "x_val.shape: (1664, 20)\n",
      "x_val.shape: (3299, 20)\n",
      "x_test.shape: (1800, 20)\n",
      "x_test.shape: (3000, 20)\n",
      "x_test.shape: (3350, 20)\n",
      "\u001b[1mt1_1\u001b[0m run-time: 0.325 days \n",
      "\u001b[1mt2_1\u001b[0m run-time: 0.105 days \n",
      "\u001b[1mt3_1\u001b[0m run-time: 0.059 days\n",
      "eta: 0.199976726035066\n",
      "shape x_train: (4222, 20)\n",
      "shape y_train: (4222, 3)\n",
      "shape x_val: (3299, 20)\n",
      "shape y_val: (3299, 3)\n",
      "shape x_test: (3350, 20)\n",
      "shape y_test: (3350, 3)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected sequence or array-like, got <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b8190ac700fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;31m# shuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weibull/lib/python3.8/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36mshuffle\u001b[0;34m(random_state, n_samples, *arrays)\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0mresample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     \"\"\"\n\u001b[0;32m--> 631\u001b[0;31m     return resample(*arrays, replace=False, n_samples=n_samples,\n\u001b[0m\u001b[1;32m    632\u001b[0m                     random_state=random_state)\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weibull/lib/python3.8/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(replace, n_samples, random_state, stratify, *arrays)\u001b[0m\n\u001b[1;32m    516\u001b[0m                                                     n_samples))\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstratify\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weibull/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \"\"\"\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weibull/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \"\"\"\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weibull/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected sequence or array-like, got <class 'int'>"
     ]
    }
   ],
   "source": [
    "#!#!#!# TRAIN #!#!#!#\n",
    "# Bearing1_1\n",
    "folder_indv_bearing = folder_raw_data_train / \"Bearing1_1\"\n",
    "date_dict = create_date_dict(folder_indv_bearing)\n",
    "df_spec, labels_dict = build_spectrogram_df_femto(\n",
    "    folder_indv_bearing, date_dict, channel_name=\"acc_horz\",\n",
    ")\n",
    "\n",
    "# create temp x, y\n",
    "x1_1, y1_1 = create_x_y(\n",
    "    df_spec, labels_dict, bucket_size, print_shape=False\n",
    ")\n",
    "x_train = x1_1\n",
    "y_train = y1_1\n",
    "\n",
    "print(\"x_train.shape:\", x_train.shape)\n",
    "\n",
    "####\n",
    "# Bearing2_1\n",
    "folder_indv_bearing = folder_raw_data_train / \"Bearing2_1\"\n",
    "date_dict = create_date_dict(folder_indv_bearing)\n",
    "df_spec, labels_dict = build_spectrogram_df_femto(\n",
    "    folder_indv_bearing, date_dict, channel_name=\"acc_horz\",\n",
    ")\n",
    "\n",
    "# create temp x, y\n",
    "x2_1, y2_1 = create_x_y(\n",
    "    df_spec, labels_dict, bucket_size, print_shape=False\n",
    ")\n",
    "x_train = np.append(x_train, x2_1, 0)\n",
    "y_train = np.append(y_train, y2_1, 0)\n",
    "\n",
    "print(\"x_train.shape:\", x_train.shape)\n",
    "\n",
    "####\n",
    "# Bearing3_1\n",
    "folder_indv_bearing = folder_raw_data_train / \"Bearing3_1\"\n",
    "date_dict = create_date_dict(folder_indv_bearing)\n",
    "df_spec, labels_dict = build_spectrogram_df_femto(\n",
    "    folder_indv_bearing, date_dict, channel_name=\"acc_horz\",\n",
    ")\n",
    "\n",
    "# create temp x, y\n",
    "x3_1, y3_1 = create_x_y(\n",
    "    df_spec, labels_dict, bucket_size, print_shape=False\n",
    ")\n",
    "x_train = np.append(x_train, x3_1, 0)\n",
    "y_train = np.append(y_train, y3_1, 0)\n",
    "\n",
    "print(\"x_train.shape:\", x_train.shape)\n",
    "\n",
    "\n",
    "##############################################################\n",
    "#!#!#!# VAL #!#!#!#\n",
    "# Bearing1_2\n",
    "folder_indv_bearing = folder_raw_data_train / \"Bearing1_2\"\n",
    "date_dict = create_date_dict(folder_indv_bearing)\n",
    "df_spec, labels_dict = build_spectrogram_df_femto(\n",
    "    folder_indv_bearing, date_dict, channel_name=\"acc_horz\",\n",
    ")\n",
    "\n",
    "# create temp x, y\n",
    "x1_2, y1_2 = create_x_y(\n",
    "    df_spec, labels_dict, bucket_size, print_shape=False\n",
    ")\n",
    "x_val = x1_2\n",
    "y_val = y1_2\n",
    "\n",
    "print(\"x_val.shape:\", x_val.shape)\n",
    "\n",
    "####\n",
    "# Bearing2_2\n",
    "folder_indv_bearing = folder_raw_data_train / \"Bearing2_2\"\n",
    "date_dict = create_date_dict(folder_indv_bearing)\n",
    "df_spec, labels_dict = build_spectrogram_df_femto(\n",
    "    folder_indv_bearing, date_dict, channel_name=\"acc_horz\",\n",
    ")\n",
    "\n",
    "# create temp x, y\n",
    "x2_2, y2_2 = create_x_y(\n",
    "    df_spec, labels_dict, bucket_size, print_shape=False\n",
    ")\n",
    "x_val = np.append(x_val, x2_2, 0)\n",
    "y_val = np.append(y_val, y2_2, 0)\n",
    "\n",
    "print(\"x_val.shape:\", x_val.shape)\n",
    "\n",
    "####\n",
    "# Bearing3_2\n",
    "folder_indv_bearing = folder_raw_data_train / \"Bearing3_2\"\n",
    "date_dict = create_date_dict(folder_indv_bearing)\n",
    "df_spec, labels_dict = build_spectrogram_df_femto(\n",
    "    folder_indv_bearing, date_dict, channel_name=\"acc_horz\",\n",
    ")\n",
    "\n",
    "# create temp x, y\n",
    "x3_2, y3_2 = create_x_y(\n",
    "    df_spec, labels_dict, bucket_size, print_shape=False\n",
    ")\n",
    "x_val = np.append(x_val, x3_2, 0)\n",
    "y_val = np.append(y_val, y3_2, 0)\n",
    "\n",
    "print(\"x_val.shape:\", x_val.shape)\n",
    "\n",
    "\n",
    "##############################################################\n",
    "#!#!#!# TEST #!#!#!#\n",
    "# Bearing1_3\n",
    "folder_indv_bearing = folder_raw_data_test / \"Bearing1_3\"\n",
    "date_dict = create_date_dict(folder_indv_bearing)\n",
    "df_spec, labels_dict = build_spectrogram_df_femto(\n",
    "    folder_indv_bearing, date_dict, channel_name=\"acc_horz\",\n",
    ")\n",
    "\n",
    "# create temp x, y\n",
    "x1_3, y1_3 = create_x_y(\n",
    "    df_spec, labels_dict, bucket_size, print_shape=False\n",
    ")\n",
    "x_test = x1_3\n",
    "y_test = y1_3\n",
    "\n",
    "print(\"x_test.shape:\", x_test.shape)\n",
    "\n",
    "####\n",
    "# Bearing2_3\n",
    "folder_indv_bearing = folder_raw_data_test / \"Bearing2_3\"\n",
    "date_dict = create_date_dict(folder_indv_bearing)\n",
    "df_spec, labels_dict = build_spectrogram_df_femto(\n",
    "    folder_indv_bearing, date_dict, channel_name=\"acc_horz\",\n",
    ")\n",
    "\n",
    "# create temp x, y\n",
    "x2_3, y2_3 = create_x_y(\n",
    "    df_spec, labels_dict, bucket_size, print_shape=False\n",
    ")\n",
    "x_test = np.append(x_test, x2_3, 0)\n",
    "y_test = np.append(y_test, y2_3, 0)\n",
    "\n",
    "print(\"x_test.shape:\", x_test.shape)\n",
    "\n",
    "####\n",
    "# Bearing3_3\n",
    "folder_indv_bearing = folder_raw_data_test / \"Bearing3_3\"\n",
    "date_dict = create_date_dict(folder_indv_bearing)\n",
    "df_spec, labels_dict = build_spectrogram_df_femto(\n",
    "    folder_indv_bearing, date_dict, channel_name=\"acc_horz\",\n",
    ")\n",
    "\n",
    "# create temp x, y\n",
    "x3_3, y3_3 = create_x_y(\n",
    "    df_spec, labels_dict, bucket_size, print_shape=False\n",
    ")\n",
    "x_test = np.append(x_test, x3_3, 0)\n",
    "y_test = np.append(y_test, y3_3, 0)\n",
    "\n",
    "print(\"x_test.shape:\", x_test.shape)\n",
    "\n",
    "\n",
    "##############################################################\n",
    "#### Weibull ####\n",
    "\n",
    "t1_1 = np.max(y1_1[:, 0])  # get the run-time in days\n",
    "t2_1 = np.max(y2_1[:, 0])\n",
    "t3_1 = np.max(y3_1[:, 0])\n",
    "\n",
    "# bold printout https://stackoverflow.com/a/17303428/9214620\n",
    "print(\n",
    "    f\"\\033[1mt1_1\\033[0m run-time: {t1_1:.3f} days \\n\\033[1mt2_1\\033[0m run-time: {t2_1:.3f} days \\n\\033[1mt3_1\\033[0m run-time: {t3_1:.3f} days\"\n",
    ")\n",
    "\n",
    "# calculate the weibull properties\n",
    "beta = 2.0 # shape parameter\n",
    "t_array = np.array([t1_1, t2_1, t3_1]) # build a time array of t\n",
    "r = 3 # number of failed units\n",
    "\n",
    "# characteristic life\n",
    "eta = (np.sum((t_array ** beta) / r)) ** (1 / beta)\n",
    "eta_beta_r = np.array([eta, beta, r])\n",
    "\n",
    "print('eta:', eta)\n",
    "\n",
    "\n",
    "##############################################################\n",
    "#######\n",
    "# Create x, y train/val/test\n",
    "#######\n",
    "print('shape x_train:', np.shape(x_train))\n",
    "print('shape y_train:', np.shape(y_train))\n",
    "\n",
    "print('shape x_val:', np.shape(x_val))\n",
    "print('shape y_val:', np.shape(y_val))\n",
    "\n",
    "print('shape x_test:', np.shape(x_test))\n",
    "print('shape y_test:', np.shape(y_test))\n",
    "\n",
    "# shuffle\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state)\n",
    "x_val, y_val = shuffle(x_val, y_val, random_state)\n",
    "\n",
    "# scale\n",
    "min_val, max_val = get_min_max(x_train)\n",
    "x_train = scaler(x_train, min_val, max_val)\n",
    "x_val = scaler(x_val, min_val, max_val)\n",
    "\n",
    "# create individual data sets for each run\n",
    "# so that we can easily trend the results\n",
    "x_train1_1 = scaler(x1_1, min_val, max_val) \n",
    "y_train1_1 = y1_1\n",
    "\n",
    "x_train2_1 = scaler(x2_1, min_val, max_val) \n",
    "y_train2_1 = y2_1\n",
    "\n",
    "x_train3_1 = scaler(x3_1, min_val, max_val) \n",
    "y_train3_1 = y3_1\n",
    "\n",
    "x_val1_2 = scaler(x1_2, min_val, max_val) \n",
    "y_val1_2 = y1_2\n",
    "\n",
    "x_val2_2 = scaler(x2_2, min_val, max_val) \n",
    "y_val2_2 = y2_2\n",
    "\n",
    "x_val3_2 = scaler(x3_2, min_val, max_val) \n",
    "y_val3_2 = y3_2\n",
    "\n",
    "x_test1_3 = scaler(x1_3, min_val, max_val) \n",
    "y_test1_3 = y1_3\n",
    "\n",
    "x_test2_3 = scaler(x2_3, min_val, max_val) \n",
    "y_test2_3 = y2_3\n",
    "\n",
    "x_test3_3 = scaler(x3_3, min_val, max_val) \n",
    "y_test3_3 = y3_3\n",
    "\n",
    "\n",
    "####\n",
    "# save as h5py files\n",
    "with h5py.File(\"x_train.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"x_train\", data=x_train)\n",
    "with h5py.File(\"y_train.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"y_train\", data=y_train)\n",
    "\n",
    "with h5py.File(\"x_val.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"x_val\", data=x_val)\n",
    "with h5py.File(\"y_val.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"y_val\", data=y_val)\n",
    "\n",
    "with h5py.File(\"x_test.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"x_test\", data=x_test)\n",
    "with h5py.File(\"y_test.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"y_test\", data=y_test)\n",
    "\n",
    "# save eta/beta\n",
    "with h5py.File(\"eta_beta_r.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"eta_beta_r\", data=eta_beta_r)\n",
    "\n",
    "# save t_array\n",
    "with h5py.File(\"t_array.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"t_array\", data=t_array)\n",
    "\n",
    "# Bearing1_1\n",
    "with h5py.File(\"x_train1_1.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"x_train1_1\", data=x_train1_1)\n",
    "with h5py.File(\"y_train1_1.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"y_train1_1\", data=y_train1_1)\n",
    "\n",
    "# Bearing2_1\n",
    "with h5py.File(\"x_train2_1.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"x_train2_1\", data=x_train2_1)\n",
    "with h5py.File(\"y_train2_1.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"y_train2_1\", data=y_train2_1)\n",
    "\n",
    "# Bearing3_1\n",
    "with h5py.File(\"x_train3_1.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"x_train3_1\", data=x_train3_1)\n",
    "with h5py.File(\"y_train3_1.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"y_train3_1\", data=y_train3_1)\n",
    "\n",
    "# Bearing1_2\n",
    "with h5py.File(\"x_val1_2.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"x_val1_2\", data=x_val1_2)\n",
    "with h5py.File(\"y_val1_2.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"y_val1_2\", data=y_val1_2)\n",
    "\n",
    "# Bearing2_2\n",
    "with h5py.File(\"x_val2_2.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"x_val2_2\", data=x_val2_2)\n",
    "with h5py.File(\"y_val2_2.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"y_val2_2\", data=y_val2_2)\n",
    "\n",
    "# Bearing3_2\n",
    "with h5py.File(\"x_val3_2.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"x_val3_2\", data=x_val3_2)\n",
    "with h5py.File(\"y_val3_2.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"y_val3_2\", data=y_val3_2)\n",
    "\n",
    "# Bearing1_3\n",
    "with h5py.File(\"x_test1_3.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"x_test1_3\", data=x_test1_3)\n",
    "with h5py.File(\"y_test1_3.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"y_test1_3\", data=y_test1_3)\n",
    "\n",
    "# Bearing2_3\n",
    "with h5py.File(\"x_test2_3.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"x_test2_3\", data=x_test2_3)\n",
    "with h5py.File(\"y_test2_3.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"y_test2_3\", data=y_test2_3)\n",
    "\n",
    "# Bearing3_3\n",
    "with h5py.File(\"x_test3_3.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(\"x_test3_3\", data=x_test3_3)\n",
    "with h5py.File(folder_processed_data / \"y_test3_3.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(folder_processed_data / \"y_test3_3\", data=y_test3_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30e174ad-a42e-4c2c-b581-1cc7245e3255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4222, 20)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc3f6110-5382-4d3a-be58-23f8a661d8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4222, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81298397-73ac-407d-9a7f-5c1bfb49e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = shuffle(x_train, y_train, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4acd215-fb07-4ab1-95e4-e73053baa8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
