{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal, fftpack\n",
    "import time\n",
    "import datetime\n",
    "import h5py\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "from src.data.data_utils import get_min_max, scaler\n",
    "from src.features.build_features import create_fft\n",
    "from src.visualization.visualize import create_time_frequency_plot, plot_freq_peaks\n",
    "# for plotly, if wanted\n",
    "# from plotly.subplots import make_subplots\n",
    "# import plotly.graph_objects as go\n",
    "# import plotly.io as pio\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# to clear outputs from cells\n",
    "from IPython.display import clear_output\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the root (parent folder) and the data folder locations\n",
    "folder_root = Path.cwd().parent # get root folder of repository\n",
    "\n",
    "folder_raw_data = folder_root / 'data/raw/IMS/' # raw data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text file for first measurement\n",
    "# first test folder location\n",
    "folder_1st = folder_raw_data / '1st_test'\n",
    "\n",
    "# can use numpy...\n",
    "d = np.loadtxt(folder_1st / '2003.10.22.12.06.24')\n",
    "\n",
    "# let's us pandas\n",
    "    # b1_ch1 - Bearing 1, channel 1\n",
    "    # b1_ch2 - Bearing 1, channel 2\n",
    "    # etc, etc ...\n",
    "\n",
    "col_names = ['b1_ch1', 'b1_ch2', 'b2_ch3', 'b2_ch4', 'b3_ch5', 'b3_ch6', 'b4_ch7', 'b4_ch8']\n",
    "df = pd.read_csv(folder_1st / '2003.10.22.12.06.24', sep='\\t', names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the shape of the dataframe?\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gousseau et al. postulate in their paper ['Analysis of the Rolling Element Bearing data set of the Center for Intelligent Maintenance Systems of theUniversity of Cincinnati'](https://hal.archives-ouvertes.fr/hal-01715193) that the actual collection frequency is 20.48 kHz.\n",
    "\n",
    "We will use this collection frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first bearing channel\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(\n",
    "    np.arange(0,df.shape[0], dtype='float64') / (20.48 * 10**3), # make x-axis in seconds\n",
    "    df['b1_ch1'] # acceleration data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# practice detrending\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 8))\n",
    "\n",
    "plt.plot(df['b1_ch1'], alpha=0.5, label='original signal')\n",
    "y_detrend = signal.detrend(df['b1_ch1'], type=\"linear\")\n",
    "plt.plot(y_detrend, alpha=0.5, label='detrended signal')\n",
    "\n",
    "# apply either a hamming or kaiser windowing function\n",
    "# y_detrend *= np.hamming(len(y_detrend))\n",
    "y_detrend *= np.kaiser(len(y_detrend), 3)\n",
    "plt.plot(y_detrend, alpha=0.5, label='windowed signal')\n",
    "plt.legend(loc='center left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a function to create the FFT (`create_fft`), and another function to plot the time and frequency domains (`create_time_frequency_plot`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fft\n",
    "x, y, xf, yf = create_fft(df, y_name='b1_ch2', sample_freq=20480.0, window='kaiser', beta=3)\n",
    "\n",
    "# plot\n",
    "create_time_frequency_plot(x, y, xf, yf, save_plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrogram\n",
    "We want to build a spectrogram of the first test set.\n",
    "\n",
    "First, get the name of all the files in the folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Naming Each Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = sorted(os.listdir(folder_1st))\n",
    "date_list[:10] # check to see it makes sense..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names are a bit clunky. Let's convert them all to UNIX timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://stackoverflow.com/a/9637908/9214620\n",
    "s = '2003.10.22.12.06.24'\n",
    "t = time.mktime(datetime.datetime.strptime(s, \"%Y.%m.%d.%H.%M.%S\").timetuple())\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.replace('.', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure it makes sense, convert back to a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.fromtimestamp(t).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate FFT for Each Signal\n",
    "We'll calculate the FFT for each signal. This will be stored in a pandas dataframe, with each new column being a new signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = '2003.10.22.12.06.24'\n",
    "start_time = time.mktime(datetime.datetime.strptime(start_time, \"%Y.%m.%d.%H.%M.%S\").timetuple())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_spectrogram_df(folder, date_list, channel_name='b3_ch5', start_time='2003.10.22.12.06.24', col_day_increment=False,\n",
    "                         col_names = ['b1_ch1', 'b1_ch2', 'b2_ch3', 'b2_ch4', 'b3_ch5', 'b3_ch6', 'b4_ch7', 'b4_ch8']):\n",
    "    '''function that builds the spectrogram data'''\n",
    "    \n",
    "    # convert start_time to unix timestamp\n",
    "    start_time = time.mktime(datetime.datetime.strptime(start_time, \"%Y.%m.%d.%H.%M.%S\").timetuple())\n",
    "\n",
    "    # instantiate dataframe for the spectrogram\n",
    "    dft = pd.DataFrame()\n",
    "       \n",
    "    # dictionary to store any labels\n",
    "    labels_dict = {}\n",
    "\n",
    "    # iterate through each date that samples were taken\n",
    "    # date_list should be sorted from earliest to latest\n",
    "    for i, sample_name in enumerate(date_list):\n",
    "        # convert sample_name to unix timestamp\n",
    "        unix_timestamp = time.mktime(datetime.datetime.strptime(sample_name, \"%Y.%m.%d.%H.%M.%S\").timetuple())\n",
    "        date_nice_format = datetime.datetime.fromtimestamp(unix_timestamp).strftime('%Y-%m-%d %H:%M:%S') # reformat date\n",
    "\n",
    "        # open the file containing the measurements\n",
    "        df = pd.read_csv(folder / sample_name, sep='\\t', names=col_names)\n",
    "\n",
    "        # create fft\n",
    "        xf, yf = create_fft(df, x_name='Time', y_name=channel_name, sample_freq=20480.0, show_plot=False, window='kaiser', beta=3)\n",
    "        # xf, yf = create_fft(df, x_name='Time', y_name=channel_name, sample_freq=20000.0, show_plot=False, window='kaiser', beta=3)\n",
    "\n",
    "        # change sample name slightly to change '.' to '_' (personal preference)\n",
    "        sample_name = sample_name.replace('.', '_')\n",
    "\n",
    "        # append the time increments\n",
    "        time_increment_seconds = unix_timestamp-start_time\n",
    "        time_increment_days = time_increment_seconds /(60 * 60 * 24)\n",
    "        \n",
    "        # create new column for the current sample_name FFT\n",
    "        if col_day_increment == False:\n",
    "            dft[date_nice_format] = yf\n",
    "        if col_day_increment == True:\n",
    "            dft[str(time_increment_days)] = yf\n",
    "\n",
    "        # create new dictionary key and values to store lable info\n",
    "        labels_dict[sample_name] = [date_nice_format, sample_name, unix_timestamp, time_increment_seconds, time_increment_days]\n",
    "\n",
    "    dft = dft.set_index(xf, drop=True) # index as frequency (Hz)\n",
    "    return dft, labels_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at bearing 1, channel 1. This bearing did not fail in this run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_1st = folder_raw_data / '1st_test'\n",
    "\n",
    "date_list = sorted(os.listdir(folder_1st))\n",
    "\n",
    "df_spec, labels_dict = build_spectrogram_df(folder_1st, date_list, channel_name='b1_ch1', start_time='2003.10.22.12.06.24')\n",
    "df_spec.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "plt.pcolormesh(df_spec.columns, df_spec.index, df_spec)\n",
    "\n",
    "ax.set_ylabel(\"Frequency (Hz)\")\n",
    "plt.xticks(df_spec.columns[::100]) # show every 100th date on x-axis ticks\n",
    "plt.xticks(rotation=75)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above spectrogram is fairly \"dim\". This is partly because the 1000 Hz peak is dominating the FFT.\n",
    "\n",
    "We'll adjust the vmax to get better clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "# set vmax to 0.01\n",
    "plt.pcolormesh(df_spec.columns, df_spec.index, df_spec, vmax=0.01)\n",
    "\n",
    "ax.set_ylabel(\"Frequency (Hz)\")\n",
    "plt.xticks(df_spec.columns[::100]) # show every 100th date on x-axis ticks\n",
    "plt.xticks(rotation=75)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better! You can see in the above spectrogram that as this bearing (bearing 1) gets closer to the end of its run, the higher frequencies get noisier and larger.\n",
    "\n",
    "Now let's look at a bearing that did have a failure. Bearing 3 had an inner race defect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_1st = folder_raw_data / '1st_test'\n",
    "\n",
    "date_list = sorted(os.listdir(folder_1st))\n",
    "\n",
    "# reminder of bearing acceleration channels\n",
    "# col_names = ['b1_ch1', 'b1_ch2', 'b2_ch3', 'b2_ch4', 'b3_ch5', 'b3_ch6', 'b4_ch7', 'b4_ch8']\n",
    "\n",
    "df_spec, labels_dict = build_spectrogram_df(folder_1st, date_list, channel_name='b3_ch5', start_time='2003.10.22.12.06.24')\n",
    "\n",
    "# plot spectrogram\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "plt.pcolormesh(df_spec.columns, df_spec.index, df_spec, vmax=0.01)\n",
    "ax.set_ylabel(\"Frequency (Hz)\")\n",
    "plt.xticks(df_spec.columns[::100]) # show every 100th date on x-axis ticks\n",
    "plt.xticks(rotation=75)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_1st = folder_raw_data / '1st_test'\n",
    "\n",
    "date_list = sorted(os.listdir(folder_1st))\n",
    "\n",
    "# reminder of bearing acceleration channels\n",
    "# col_names = ['b1_ch1', 'b1_ch2', 'b2_ch3', 'b2_ch4', 'b3_ch5', 'b3_ch6', 'b4_ch7', 'b4_ch8']\n",
    "\n",
    "df_spec, labels_dict = build_spectrogram_df(folder_1st, date_list, channel_name='b3_ch6', start_time='2003.10.22.12.06.24')\n",
    "\n",
    "# plot spectrogram\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "plt.pcolormesh(df_spec.columns, df_spec.index, df_spec, vmax=0.01)\n",
    "ax.set_ylabel(\"Frequency (Hz)\")\n",
    "plt.xticks(df_spec.columns[::100]) # show every 100th date on x-axis ticks\n",
    "plt.xticks(rotation=75)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoom in a bit so that we are only looking at frequencies less than 1500 Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_spec[df_spec.index < 1500]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "plt.pcolormesh(df_temp.columns, df_temp.index, df_temp, vmax=0.01)\n",
    "\n",
    "ax.set_ylabel(\"Frequency (Hz)\")\n",
    "plt.xticks(df_spec.columns[::100])\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at dates closer to the end of the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = sorted(os.listdir(folder_1st))\n",
    "date_list = date_list[int((1-len(date_list)*0.2/len(date_list))*len(date_list)):] # get the last 20% of dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_1st = folder_raw_data / '1st_test'\n",
    "\n",
    "# reminder of bearing acceleration channels\n",
    "# col_names = ['b1_ch1', 'b1_ch2', 'b2_ch3', 'b2_ch4', 'b3_ch5','float' object has no attribute 'DataFrame'df_spec 'b3_ch6', 'b4_ch7', 'b4_ch8']\n",
    "\n",
    "df_spec, labels_dict = build_spectrogram_df(folder_1st, date_list, channel_name='b3_ch6', start_time=date_list[0])\n",
    "\n",
    "df_temp = df_spec[df_spec.index < 400]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "plt.pcolormesh(df_temp.columns, df_temp.index, df_temp, vmax=0.013)\n",
    "\n",
    "ax.set_ylabel(\"Frequency (Hz)\")\n",
    "plt.xticks(df_spec.columns[::20])\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Rexnord ZA-2115 Charachteristics           |          |          |\n",
    "| :----------------------------------------- | -------- | -------- |\n",
    "| Pitch diameter ($P_D$)                     | 2.815 in | 71.5 mm  |\n",
    "| Rolling element diameter ($B_D$)           | 0.331 in | 8.4 mm   |\n",
    "| Number of rolling elements per row ($N_B$) | 16       | 16       |\n",
    "| Contact angle ($\\phi$)                    | 15.7°    | 15.7°    |\n",
    "| Static load                                | 6000 lbs | 26690 N  |\n",
    "| **Shaft RPM**                              | 2000 RPM | 2000 RPM |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdia = 2.815 # pitch diameter (in)\n",
    "bdia = 0.331 # rolling ball diameter (in)\n",
    "nb = 16.0 # no. or rolling elements per row\n",
    "phi = 15.7 * np.pi / 180 # contact angle (in radians)\n",
    "load = 6000 # load, in lbs\n",
    "rps = 2000 / 60.0 # rotations per second (Hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to calculate what the important frequencies for the bearing are.\n",
    "\n",
    "The shaft frequency is 33.3 Hz.\n",
    "\n",
    "Therefore, the fundamental train frequency (or cage failing frequency) is:\n",
    "\n",
    "$$\\text{FTF} = \\frac{\\text{RPS}}{2} (1 - \\frac{B_D}{P_D} \\cos{\\phi})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftf = rps/2 * (1 - bdia/pdia * np.cos(phi))\n",
    "print('FTF =', ftf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ball pass frequency of the inner race:\n",
    "\n",
    "$$\\text{BPFI} = \\frac{N_B}{2} \\text{RPS} (1 + \\frac{B_D}{P_D} \\cos{\\phi})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpfi = nb/2 * rps * (1 + bdia/pdia * np.cos(phi))\n",
    "print('BPFI =', bpfi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ball pass frequency of the outer race:\n",
    "\n",
    "$$\\text{BPFO} = \\frac{N_B}{2} \\text{RPS} (1 - \\frac{B_D}{P_D} \\cos{\\phi})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpfo = nb/2 * rps * (1 - bdia/pdia * np.cos(phi))\n",
    "print('BPFO =', bpfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ball spin frequency:\n",
    "\n",
    "$$\\text{BSF} = \\frac{P_D}{2 B_D} \\text{RPS} (1 - (\\frac{B_D}{P_D} \\cos{\\phi})^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsf = pdia/(2*bdia) * rps * (1 - (bdia/pdia * np.cos(phi))**2)\n",
    "print('BSF =', bsf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = sorted(os.listdir(folder_1st))\n",
    "\n",
    "df_spec, labels_dict = build_spectrogram_df(folder_1st, date_list, channel_name='b3_ch6', start_time=date_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_size = 200\n",
    "\n",
    "df_temp = df_spec.iloc[:10000]\n",
    "a = np.array(df_temp) # make numpy array\n",
    "\n",
    "# get the y-axis (frequency values)\n",
    "y = np.array(df_temp.index)\n",
    "y = np.max(y.reshape(-1,bucket_size),axis=1)\n",
    "\n",
    "# get the max value for each bucket\n",
    "# https://stackoverflow.com/a/15956341/9214620\n",
    "max_a = np.max(a.reshape(-1,bucket_size,2156),axis=1)\n",
    "\n",
    "print('shape of max_a array:', np.shape(max_a))\n",
    "\n",
    "# get the mean value for each bucket\n",
    "avg_a = np.mean(a.reshape(-1,bucket_size,2156),axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "plt.pcolormesh(df_temp.columns, y, max_a)\n",
    "ax.set_ylabel(\"Frequency (Hz)\")\n",
    "plt.xticks(df_temp.columns[::200])\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(max_a.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at 3rd Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_3rd = folder_raw_data / '3rd_test'\n",
    "date_list = sorted(os.listdir(folder_3rd))\n",
    "\n",
    "# reminder of bearing acceleration channels\n",
    "# col_names = ['b1_ch1', 'b2_ch2', 'b3_ch3', 'b4_ch4']\n",
    "\n",
    "col_names = ['b1_ch1', 'b2_ch2', 'b3_ch3', 'b4_ch4']\n",
    "\n",
    "df_spec, labels_dict = build_spectrogram_df(folder_3rd, date_list, channel_name='b3_ch3', start_time=date_list[0], col_names=col_names)\n",
    "\n",
    "df_temp = df_spec\n",
    "# df_temp = df_spec[df_spec.index < 400]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "\n",
    "plt.pcolormesh(np.array(df_temp), \n",
    "               vmax=0.013\n",
    "              )\n",
    "# plt.pcolormesh(df_temp.columns, df_temp.index, df_temp, \n",
    "#                vmax=0.013\n",
    "#               )\n",
    "\n",
    "# ax.set_ylabel(\"Frequency (Hz)\")\n",
    "# plt.xticks(df_spec.columns[::50])\n",
    "# plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_3rd = folder_raw_data / '3rd_test'\n",
    "date_list = sorted(os.listdir(folder_3rd))\n",
    "\n",
    "# reminder of bearing acceleration channels\n",
    "# col_names = ['b1_ch1', 'b2_ch2', 'b3_ch3', 'b4_ch4']\n",
    "\n",
    "col_names = ['b1_ch1', 'b2_ch2', 'b3_ch3', 'b4_ch4']\n",
    "\n",
    "df_spec, labels_dict = build_spectrogram_df(folder_3rd, date_list, channel_name='b1_ch1', start_time=date_list[0], col_names=col_names)\n",
    "\n",
    "df_temp = df_spec\n",
    "# df_temp = df_spec[df_spec.index < 400]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "\n",
    "plt.pcolormesh(np.array(df_temp), \n",
    "               vmax=0.013\n",
    "              )\n",
    "# plt.pcolormesh(df_temp.columns, df_temp.index, df_temp, \n",
    "#                vmax=0.013\n",
    "#               )\n",
    "\n",
    "# ax.set_ylabel(\"Frequency (Hz)\")\n",
    "# plt.xticks(df_spec.columns[::50])\n",
    "# plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for t in labels_dict.keys():\n",
    "    x.append(labels_dict[t][-1])\n",
    "\n",
    "x = np.sort(np.array(x))\n",
    "\n",
    "t = x[-1]\n",
    "print('run-time-to-failure:', f'{t:.3f} days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WeiBayes Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![weibayes](weibayes.png)\n",
    "\n",
    "Calculate the $\\eta$.\n",
    "\n",
    "We will assume a $\\beta$ of 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the 2nd test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_2nd = folder_raw_data / '2nd_test'\n",
    "date_list = sorted(os.listdir(folder_2nd))\n",
    "\n",
    "# reminder of bearing acceleration channels\n",
    "# col_names = ['b1_ch1', 'b2_ch2', 'b3_ch3', 'b4_ch4']\n",
    "\n",
    "col_names = ['b1_ch1', 'b2_ch2', 'b3_ch3', 'b4_ch4']\n",
    "\n",
    "df_spec, labels_dict = build_spectrogram_df(folder_2nd, date_list, channel_name='b1_ch1', start_time=date_list[0], col_names=col_names)\n",
    "\n",
    "df_temp = df_spec\n",
    "# df_temp = df_spec[df_spec.index < 400]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "plt.pcolormesh(df_temp.columns, df_temp.index, df_temp, \n",
    "               vmax=0.013\n",
    "              )\n",
    "\n",
    "ax.set_ylabel(\"Frequency (Hz)\")\n",
    "plt.xticks(df_spec.columns[::50])\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 2nd test set, there are four (4) bearings. They are run until one failes (bearing 1).\n",
    "\n",
    "We need to find the total run-time-to-failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for t in labels_dict.keys():\n",
    "    x.append(labels_dict[t][-1])\n",
    "\n",
    "x = np.sort(np.array(x))\n",
    "\n",
    "t = x[-1]\n",
    "print('run-time-to-failure:', f'{t:.3f} days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 2.0 # shape parameter\n",
    "r = 1 # number of failed bearings\n",
    "i = 4 # number of bearings\n",
    "\n",
    "t_array = np.repeat(t, i) # build a time array of t\n",
    "t_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The characteristic life, $\\eta$ (eta), is defined as the age at which 63.2% of the units will have failed, the B63.2 life.\" -- from The New Weibull Handbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# characteristic life\n",
    "eta = (np.sum(t_array ** beta) / 1) ** (1 / beta)\n",
    "print(eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the characteristic life, we can build a Weibull distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "pal = sns.cubehelix_palette(6, rot=-.25, light=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_pdf(t, eta, beta):\n",
    "    \"weibull PDF function\"\n",
    "    return (beta/(eta ** beta))*(t**(beta-1.0))*np.exp(-1.0*((t/eta)**beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0,50,1000)\n",
    "f = weibull_pdf(t, eta, beta)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(4,3), dpi=150)\n",
    "axes.title.set_text('Weibull Distribution PDF')\n",
    "axes.set_xlabel('Time (t)')\n",
    "axes.set_ylabel('F(t)')\n",
    "axes.grid(False)\n",
    "axes.axes.xaxis.set_ticks([])\n",
    "axes.axes.yaxis.set_ticks([])\n",
    "\n",
    "plt.plot(t, f, color=pal[5], linewidth=2)\n",
    "# plt.savefig('weibull.png',format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_cdf(t, eta, beta):\n",
    "    \"weibull CDF function\"\n",
    "    return (1.0 - np.exp(-1.0*((t/eta)**beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0,50,1000)\n",
    "F = weibull_cdf(t, eta, beta)\n",
    "\n",
    "plt.plot(t, F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Features for 2nd Test\n",
    "\n",
    "We will now build the features from the 2nd test set. This is what we'll use to train the model on. \n",
    "\n",
    "- **Training Set:** 2nd test data\n",
    "- **Validation Set:** 3rd test data\n",
    "- **Test Set:** 1st test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_2nd = folder_raw_data / '2nd_test'\n",
    "# date_list = sorted(os.listdir(folder_2nd))\n",
    "\n",
    "# folder_3rd = folder_raw_data / '3rd_test'\n",
    "# date_list = sorted(os.listdir(folder_3rd))\n",
    "# col_names = ['b1_ch1', 'b2_ch2', 'b3_ch3', 'b4_ch4']\n",
    "\n",
    "folder_1st = folder_raw_data / '1st_test'\n",
    "date_list = sorted(os.listdir(folder_1st))\n",
    "col_names = ['b1_ch1', 'b1_ch2', 'b2_ch3', 'b2_ch4', 'b3_ch5','b3_ch6', 'b4_ch7', 'b4_ch8']\n",
    "# df_spec, labels_dict = build_spectrogram_df(folder_1st, date_list, channel_name='b3_ch6', start_time=date_list1[0], col_names=col_names)\n",
    "\n",
    "# reminder of bearing acceleration channels\n",
    "# col_names = ['b1_ch1', 'b2_ch2', 'b3_ch3', 'b4_ch4']\n",
    "\n",
    "# col_names = ['b1_ch1', 'b2_ch2', 'b3_ch3', 'b4_ch4']\n",
    "\n",
    "df_spec, labels_dict = build_spectrogram_df(folder_1st, date_list, channel_name='b3_ch6', start_time=date_list[0], col_names=col_names)\n",
    "df_spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = df_spec.shape[1]\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0,10.0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_size = 500\n",
    "\n",
    "samples = df_spec.shape[1]\n",
    "\n",
    "df_temp = df_spec.iloc[:10000]\n",
    "a = np.array(df_temp) # make numpy array\n",
    "print(np.shape(a))\n",
    "\n",
    "# get the y-axis (frequency values)\n",
    "y = np.array(df_temp.index)\n",
    "y = np.max(y.reshape(-1,bucket_size),axis=1)\n",
    "y = list(y.round().astype('int')[::2])\n",
    "y.insert(0,0)\n",
    "\n",
    "# get the max value for each bucket\n",
    "# https://stackoverflow.com/a/15956341/9214620\n",
    "max_a = np.max(a.reshape(-1,bucket_size,samples),axis=1)\n",
    "\n",
    "print('shape of max_a array:', np.shape(max_a))\n",
    "\n",
    "# get the mean value for each bucket\n",
    "avg_a = np.mean(a.reshape(-1,bucket_size,samples),axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "# plt.pcolormesh(avg_a)\n",
    "plt.pcolormesh(max_a)\n",
    "# plt.pcolormesh(y=df_temp.columns, c=max_a)\n",
    "# ax.set_ylabel(\"Frequency (Hz)\")\n",
    "\n",
    "# labels = [item.get_text() for item in ax.get_yticklabels()]\n",
    "loc = ax.get_yticks()\n",
    "labels = ax.get_yticklabels()\n",
    "# labels[1] = 'Testing'\n",
    "# ax.set_yticklabels(y)\n",
    "# plt.yticks(loc=np.arange(0,11.0,1), labels=y)\n",
    "# plt.xticks(list(df_temp.columns[::200]))\n",
    "# plt.xticks(rotation=45)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Frequency Bucket')\n",
    "plt.savefig('spectrogram_set1.png',format='png', bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the last two samples suddenly drop in value, just before the test is stopped. We should remove these before putting them in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the last several runs, from one of the buckets\n",
    "plt.plot(max_a.T[960:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_a.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(max_a.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think we will input a vector into the NN: the 10 values from bucket $t_n$, using the maximum value from each bucket.\n",
    "\n",
    "$x$ will be the vector of 10 values. $y$ will be the time value, measured in days.\n",
    "\n",
    "Create the x_train and y_train sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_x_y(df_spec, labels_dict,  bucket_size=1000, print_shape=False):\n",
    "\n",
    "    samples = df_spec.shape[1]\n",
    "\n",
    "    df_temp = df_spec.iloc[:10000]\n",
    "    a = np.array(df_temp) # make numpy array\n",
    "\n",
    "    # get the max value for each bucket\n",
    "    # https://stackoverflow.com/a/15956341/9214620\n",
    "    x = np.max(a.reshape(-1,bucket_size,samples),axis=1).T\n",
    "\n",
    "    temp_days = []\n",
    "    for i in labels_dict.keys():\n",
    "        temp_days.append(labels_dict[i][-1])\n",
    "\n",
    "    temp_days = np.sort(np.array(temp_days))\n",
    "\n",
    "    run_time = np.max(temp_days)\n",
    "\n",
    "    # turn into percentage life left\n",
    "    y = []\n",
    "    for i in temp_days:\n",
    "        y.append([i, i/run_time, run_time-i])\n",
    "\n",
    "    y = np.array(y)\n",
    "\n",
    "    # drop the last two values from the x and y, since they seem to be erroneous\n",
    "    x = x[:len(x)-2]\n",
    "    y = y[:len(y)-2]\n",
    "    \n",
    "    if print_shape == True:\n",
    "        print('Shape of x:', np.shape(x))\n",
    "        print('Shape of y:', np.shape(y))\n",
    "        print('run-time-to-failure:', f'{run_time:.3f} days')\n",
    "    return x, y   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = create_x_y(df_spec, labels_dict,  bucket_size=1000, print_shape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:,2][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the last several values in the y array\n",
    "y[::-1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x[930:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_x_y(df_spec, labels_dict,  bucket_size=1000)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_x_y(df_spec, labels_dict,  bucket_size=1000)\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(X)\n",
    "\n",
    "X = min_max_scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test scaling\n",
    "x, y = create_x_y(df_spec, labels_dict,  bucket_size=1000)\n",
    "\n",
    "def get_min_max(x):\n",
    "\n",
    "    # flatten the input array http://bit.ly/2MQuXZd\n",
    "    flat_vector = np.concatenate(x).ravel()\n",
    "\n",
    "    min_val = min(flat_vector)\n",
    "    max_val = max(flat_vector)\n",
    "\n",
    "    return min_val, max_val\n",
    "\n",
    "min_val, max_val = get_min_max(x)\n",
    "\n",
    "def scaler(x, min_val, max_val, lower_norm_val=0, upper_norm_val=1):\n",
    "    \"\"\"Scale the signal between a min and max value\n",
    "    \n",
    "    Parameters\n",
    "    ===========\n",
    "    x : ndarray\n",
    "        Signal that is being normalized\n",
    "\n",
    "    max_val : int or float\n",
    "        Maximum value of the signal or dataset\n",
    "\n",
    "    min_val : int or float\n",
    "        Minimum value of the signal or dataset\n",
    "\n",
    "    lower_norm_val : int or float\n",
    "        Lower value you want to normalize the data between (e.g. 0)\n",
    "\n",
    "    upper_norm_val : int or float\n",
    "        Upper value you want to normalize the data between (e.g. 1)\n",
    "\n",
    "    Returns\n",
    "    ===========\n",
    "    x : ndarray\n",
    "        Returns a new array that was been scaled between the upper_norm_val\n",
    "        and lower_norm_val values\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # https://codereview.stackexchange.com/questions/185785/scale-numpy-array-to-certain-range\n",
    "    col, row = np.shape(x)\n",
    "    for i in range(col):\n",
    "        x[i] = np.interp(x[i], (min_val, max_val), (lower_norm_val, upper_norm_val))\n",
    "    return x\n",
    "\n",
    "x = scaler(x, min_val, max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get X and y\n",
    "x, y = create_x_y(df_spec, labels_dict,  bucket_size=1000)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=13)\n",
    "\n",
    "min_val, max_val = get_min_max(x_train)\n",
    "x_train = scaler(x_train, min_val, max_val)\n",
    "x_test = scaler(x_test, min_val, max_val)\n",
    "\n",
    "\n",
    "# # scale X_train, and apply same trasnformation to X_test\n",
    "# min_max_scaler = MinMaxScaler()\n",
    "# min_max_scaler.fit(x_train)\n",
    "# x_train = min_max_scaler.transform(x_train)\n",
    "# x_test = min_max_scaler.transform(x_test)\n",
    "\n",
    "# convert to pytorch tensors\n",
    "x_train = torch.Tensor(x_train)\n",
    "x_test = torch.Tensor(x_test)\n",
    "y_train = torch.Tensor(y_train)\n",
    "y_test = torch.Tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Super Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a super small train/test set in order to \n",
    "x, y = create_x_y(df_spec, labels_dict,  bucket_size=1000)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=13)\n",
    "min_val, max_val = get_min_max(x_train)\n",
    "\n",
    "index_to_keep = np.array([2,400,980])\n",
    "\n",
    "x_train = x[index_to_keep]\n",
    "y_train = y[index_to_keep]\n",
    "# y_train = y_train[:,1]\n",
    "\n",
    "x_train = scaler(x_train, min_val, max_val)\n",
    "\n",
    "x_train = torch.Tensor(x_train)\n",
    "y_train = torch.reshape(torch.Tensor(y_train[:,1]),(-1,1))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=10, out_features=16)\n",
    "        self.fc2 = nn.Linear(in_features=16, out_features=16)\n",
    "        self.output = nn.Linear(in_features=16, out_features=1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "    # MSE loss function\n",
    "    def mse(self, y_hat, y):\n",
    "        return torch.mean((y_hat - y)**2) # y_hat is the prediction\n",
    "    \n",
    "net = Net()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 100\n",
    "loss_arr = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    y_hat = net.forward(x_train)\n",
    "#     print(y_hat.shape)\n",
    "    loss = net.mse(y_hat, y_train)\n",
    "#     loss = criterion(y_hat, y_train)\n",
    "    loss_arr.append(loss)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch: {i} Loss: {loss}')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val in x_train:\n",
    "        y_hat = net.forward(val)\n",
    "        preds.append(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Weibull CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload data \n",
    "if True:\n",
    "    folder_2nd = folder_raw_data / '2nd_test'\n",
    "    date_list2 = sorted(os.listdir(folder_2nd))\n",
    "    col_names = ['b1_ch1', 'b2_ch2', 'b3_ch3', 'b4_ch4']\n",
    "    df_spec2, labels_dict2 = build_spectrogram_df(folder_2nd, date_list2, channel_name='b1_ch1', start_time=date_list2[0], col_names=col_names)\n",
    "    ####\n",
    "    \n",
    "    \n",
    "    folder_3rd = folder_raw_data / '3rd_test'\n",
    "    date_list3 = sorted(os.listdir(folder_3rd))\n",
    "    col_names = ['b1_ch1', 'b2_ch2', 'b3_ch3', 'b4_ch4']\n",
    "    df_spec3, labels_dict3 = build_spectrogram_df(folder_3rd, date_list3, channel_name='b3_ch3', start_time=date_list3[0], col_names=col_names)\n",
    "    ####\n",
    "    \n",
    "    \n",
    "    folder_1st = folder_raw_data / '1st_test'\n",
    "    date_list1 = sorted(os.listdir(folder_1st))\n",
    "    col_names = ['b1_ch1', 'b1_ch2', 'b2_ch3', 'b2_ch4', 'b3_ch5','b3_ch6', 'b4_ch7', 'b4_ch8']\n",
    "    df_spec1, labels_dict1 = build_spectrogram_df(folder_1st, date_list1, channel_name='b3_ch6', start_time=date_list1[0], col_names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_cdf(t, eta, beta):\n",
    "    \"weibull CDF function\"\n",
    "    return (1.0 - np.exp(-1.0*((t/eta)**beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[4]*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2, y2 = create_x_y(df_spec2, labels_dict2,  bucket_size=1000, print_shape=False)\n",
    "x3, y3 = create_x_y(df_spec3, labels_dict3,  bucket_size=1000, print_shape=False)\n",
    "t2 = np.max(y2[:,0])\n",
    "t3 = np.max(y3[:,0])\n",
    "# bold printout https://stackoverflow.com/a/17303428/9214620\n",
    "print(f'\\033[1mTest 2\\033[0m run-time: {t2:.3f} days \\t\\t\\033[1mTest 3\\033[0m run-time: {t3:.3f} days')\n",
    "\n",
    "beta = 2.0 # shape parameter\n",
    "r = 2 # number of failed bearings\n",
    "i = 8 # number of bearings\n",
    "\n",
    "t_array = np.append([t2]*4, [t3]*4) # build a time array of t\n",
    "\n",
    "# characteristic life\n",
    "eta = (np.sum(t_array ** beta) / 1) ** (1 / beta)\n",
    "print(eta)\n",
    "\n",
    "t = np.linspace(0,200,1000)\n",
    "F = weibull_cdf(t, eta, beta)\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    1, 1, figsize=(8,6), \n",
    "#     dpi=150, constrained_layout=True\n",
    ")\n",
    "\n",
    "plt.plot(t, F)\n",
    "plt.ylabel('Occurence CDF')\n",
    "plt.xlabel('Bearing Life (days)')\n",
    "plt.savefig('cdf.png',format='png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, feat_in):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feat_in = feat_in\n",
    "        \n",
    "        # define the layers\n",
    "        # [features_in, hidden_layer1_features, ..., features_out]\n",
    "        feat_in, h1, h2, h3, feat_out = [self.feat_in , 64, 64, 64, 1]\n",
    "        \n",
    "        self.fc1 = nn.Linear(feat_in, h1)\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.fc3 = nn.Linear(h2, h3)\n",
    "        self.fc4 = nn.Linear(h3, feat_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "       \n",
    "        return x\n",
    "    \n",
    "    # MSE loss function\n",
    "    def mse(self, y_hat, y):\n",
    "        return torch.mean((y_hat - y)**2) # y_hat is the prediction\n",
    "    \n",
    "    def rmse(self, y_hat, y):\n",
    "        return torch.sqrt(torch.mean((y_hat - y)**2)) # y_hat is the prediction\n",
    "    \n",
    "    def rmsle(self, y_hat, y):\n",
    "        return torch.sqrt(torch.mean((torch.log(y_hat+1) - torch.log(y+1))**2))\n",
    "    \n",
    "    # Weibull loss function\n",
    "    def weibull_loss(self, y_hat, y, y_days, lambda_mod, eta, beta):\n",
    "        '''\n",
    "        y_hat       : predicted RUL\n",
    "        y           : true RUL\n",
    "        y_days      : true age (in days)\n",
    "        lambda_mod  : lambda modifier\n",
    "        eta         : characteristic life\n",
    "        beta        : shape parameter for weibull\n",
    "        '''\n",
    "        y_hat_days = (y_days + y) - y_hat\n",
    "        \n",
    "        # remove any \"inf\" values from when divided by zero\n",
    "        y_hat_days = y_hat_days[torch.isfinite(y_hat_days)]\n",
    "               \n",
    "        def weibull_cdf(t, eta, beta):\n",
    "            \"weibull CDF function\"\n",
    "            return (1.0 - torch.exp(-1.0*((t/eta)**beta)))\n",
    "        \n",
    "        cdf = weibull_cdf(y_days, eta, beta)\n",
    "        cdf_hat = weibull_cdf(y_hat_days, eta, beta)\n",
    "        \n",
    "        return lambda_mod * torch.sqrt(torch.mean(cdf_hat - cdf)**2)\n",
    "    \n",
    "    \n",
    "net = Net(20)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_pass(net, x, y, y_days, train=False, loss_func='mse', lambda_mod=1.0, eta=13.0, beta=2.0):\n",
    "    '''Similar to Sentdex tutorial\n",
    "    https://pythonprogramming.net/analysis-visualization-deep-learning-neural-network-pytorch/\n",
    "    '''\n",
    "    if train:\n",
    "        net.zero_grad()\n",
    "                \n",
    "    y_hat = net(x)\n",
    "        \n",
    "    if loss_func == 'mse':\n",
    "        loss = net.mse(y_hat, y)\n",
    "    if loss_func == 'rmse':\n",
    "        loss = net.rmse(y_hat, y)\n",
    "    if loss_func == 'rmsle':\n",
    "        loss = net.rmsle(y_hat, y)    \n",
    "    if loss_func == 'weibull':\n",
    "        rmsle_loss = net.rmsle(y_hat, y)\n",
    "        weibull_loss = net.weibull_loss(y_hat, y, y_days, lambda_mod, eta, beta)\n",
    "#         print(f'RMSLE {rmsle_loss:.3f}, Weibull Loss {weibull_loss:.3f}')\n",
    "        loss = rmsle_loss + weibull_loss\n",
    "    if loss_func == 'weibull_only':       \n",
    "        loss = net.weibull_loss(y_hat, y, y_days, lambda_mod, eta, beta)    \n",
    "\n",
    "    if train:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import EarlyStopping\n",
    "from utils import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, x_train, y_train, y_train_days,  \n",
    "          x_val, y_val, y_val_days, \n",
    "          loss_func='mse', batch_size=100, epochs=500, patience=7, \n",
    "          lambda_mod=1.0, eta=13.0, beta=2.0, early_stop_delay=20):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=False, early_stop_delay=early_stop_delay)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # track the training/validation losses during epoch\n",
    "        train_losses = []\n",
    "        train_losses_mse = []\n",
    "        \n",
    "        #############\n",
    "        # train model\n",
    "        #############\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "\n",
    "            # create the batches and send to GPU (or CPU)\n",
    "            # I don't like how we don't shuffle the data before each epoch\n",
    "            # will have to look at the data-loader documentation and implement\n",
    "            batch_x = x_train[i:i+batch_size].to(device)\n",
    "            batch_y = y_train[i:i+batch_size].to(device)\n",
    "            batch_y_days = y_train_days[i:i+batch_size].to(device)\n",
    "            \n",
    "            # train and calculate the losses\n",
    "            loss = fwd_pass(net, batch_x, batch_y, batch_y_days, train=True, loss_func=loss_func, lambda_mod=lambda_mod, eta=eta, beta=beta)\n",
    "            loss_mse = fwd_pass(net, batch_x, batch_y, batch_y_days, train=True, loss_func='mse', lambda_mod=lambda_mod, eta=eta, beta=beta)\n",
    "            train_losses.append(loss.item())\n",
    "            train_losses_mse.append(loss_mse.item())\n",
    "\n",
    "    \n",
    "        ################\n",
    "        # validate model\n",
    "        ################\n",
    "        val_loss = fwd_pass(net, x_val.to(device), y_val.to(device), y_val_days.to(device), loss_func=loss_func, lambda_mod=lambda_mod, eta=eta, beta=beta)\n",
    "        val_loss_mse = fwd_pass(net, x_val.to(device), y_val.to(device), y_val_days.to(device), loss_func='mse', lambda_mod=lambda_mod, eta=eta, beta=beta)\n",
    "        \n",
    "        loss_avg = np.mean(train_losses)\n",
    "        loss_avg_mse = np.mean(train_losses_mse)\n",
    "        \n",
    "        # save the results to a pandas dataframe\n",
    "        df = df.append(pd.DataFrame([[epoch+1,loss_avg, val_loss.item(),loss_avg_mse, val_loss_mse.item()]],\n",
    "                                    columns=['epoch', 'loss', 'val_loss', 'loss_mse', 'val_loss_mse']))\n",
    "        \n",
    "        early_stopping(val_loss, net)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "        # print out the epoch, loss, and iteration number every 5th epoch\n",
    "        if epoch % 200 == 0:\n",
    "            print(f\"Epoch: {epoch} \\tLoss: {loss_avg:.4f} \\tVal Loss: {val_loss:.4f}\")\n",
    "            \n",
    "    # load the last checkpoint with the best model\n",
    "    print('Load best model')\n",
    "    net.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    \n",
    "    df = df.reset_index(drop=True)\n",
    "            \n",
    "    return df, net\n",
    "\n",
    "def test(net, x_test, batch_size=BATCH_SIZE):\n",
    "    with torch.no_grad():  \n",
    "\n",
    "        y_hats = []\n",
    "        \n",
    "        for i in range(0, len(x_test), BATCH_SIZE):\n",
    "            batch_x = x_test[i:i+BATCH_SIZE].to(device)\n",
    "            outputs = net(batch_x)\n",
    "            y_hats.append(np.array(outputs.cpu()).reshape(-1,1))\n",
    "    \n",
    "    return np.concatenate(y_hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload data \n",
    "if False:\n",
    "    folder_2nd = folder_raw_data / '2nd_test'\n",
    "    date_list2 = sorted(os.listdir(folder_2nd))\n",
    "    col_names = ['b1_ch1', 'b2_ch2', 'b3_ch3', 'b4_ch4']\n",
    "    df_spec2, labels_dict2 = build_spectrogram_df(folder_2nd, date_list2, channel_name='b1_ch1', start_time=date_list2[0], col_names=col_names)\n",
    "    ####\n",
    "    \n",
    "    \n",
    "    folder_3rd = folder_raw_data / '3rd_test'\n",
    "    date_list3 = sorted(os.listdir(folder_3rd))\n",
    "    col_names = ['b1_ch1', 'b2_ch2', 'b3_ch3', 'b4_ch4']\n",
    "    df_spec3, labels_dict3 = build_spectrogram_df(folder_3rd, date_list3, channel_name='b3_ch3', start_time=date_list3[0], col_names=col_names)\n",
    "    ####\n",
    "    \n",
    "    \n",
    "    folder_1st = folder_raw_data / '1st_test'\n",
    "    date_list1 = sorted(os.listdir(folder_1st))\n",
    "    col_names = ['b1_ch1', 'b1_ch2', 'b2_ch3', 'b2_ch4', 'b3_ch5','b3_ch6', 'b4_ch7', 'b4_ch8']\n",
    "    df_spec1, labels_dict1 = build_spectrogram_df(folder_1st, date_list1, channel_name='b3_ch6', start_time=date_list1[0], col_names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rul_or_percent = 'rul'\n",
    "rul_or_percent = 'percent'\n",
    "bucket_size = 500\n",
    "\n",
    "# get X and y\n",
    "x2, y2 = create_x_y(df_spec2, labels_dict2,  bucket_size=bucket_size, print_shape=False)\n",
    "x2 = x2[1:] # get rid of the 'zero' so that we don't have inf in weibull loss func\n",
    "y2 = y2[1:]\n",
    "\n",
    "x3, y3 = create_x_y(df_spec3, labels_dict3,  bucket_size=bucket_size, print_shape=False)\n",
    "x3 = x3[1:] # get rid of the 'zero' so that we don't have inf in weibull loss func\n",
    "y3 = y3[1:]\n",
    "\n",
    "x_train = np.append(x2, x3,0)\n",
    "y_train = np.append(y2, y3, 0)\n",
    "\n",
    "# x_train, y_train = create_x_y(df_spec2, labels_dict2,  bucket_size=bucket_size)\n",
    "# x_train = x_train[1:] # get rid of the 'zero' so that we don't have inf in weibull loss func\n",
    "# y_train = y_train[1:]\n",
    "\n",
    "x_val, y_val = create_x_y(df_spec1, labels_dict1,  bucket_size=bucket_size)\n",
    "x_val = x_val[1:]\n",
    "y_val = y_val[1:]\n",
    "\n",
    "\n",
    "# convert to pytorch tensors\n",
    "x_train = torch.Tensor(x_train)\n",
    "x_val = torch.Tensor(x_val)\n",
    "print(x_train.shape)\n",
    "\n",
    "y_train_days = torch.reshape(torch.Tensor(y_train[:,0]),(-1,1))\n",
    "y_val_days = torch.reshape(torch.Tensor(y_val[:,0]),(-1,1))\n",
    "\n",
    "\n",
    "if rul_or_percent == 'rul':\n",
    "    y_train = torch.reshape(torch.Tensor(y_train[:,2]),(-1,1))\n",
    "    print(y_train.shape)\n",
    "    y_val = torch.reshape(torch.Tensor(y_val[:,2]),(-1,1))\n",
    "else:\n",
    "    y_train = torch.reshape(torch.Tensor(y_train[:,1]),(-1,1))\n",
    "    print(y_train.shape)\n",
    "    y_val = torch.reshape(torch.Tensor(y_val[:,1]),(-1,1))\n",
    "    \n",
    "print(x_train.shape)\n",
    "print(x_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(x_train.shape[1])\n",
    "\n",
    "# setup to run on GPU (or CPU if GPU not avail.)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Running on GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on CPU\")\n",
    "    \n",
    "    \n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 500\n",
    "patience = 20\n",
    "early_stop = 0\n",
    "\n",
    "# SET TOTAL NUMBER OF EPOCHS FOR ALL EXPERIMENTS\n",
    "\n",
    "\n",
    "net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# save the results in a dataframe\n",
    "# df = train(net, x_train, y_train, y_train_days, x_test, y_test, y_test_days,  'mse', lambda_mod=3.0, eta=eta, beta=beta)\n",
    "df, net = train(net, x_train, y_train, y_train_days,  \n",
    "          x_val, y_val, y_val_days, \n",
    "            loss_func='mse', batch_size=BATCH_SIZE, epochs=EPOCHS, patience=patience, \n",
    "          lambda_mod=2.0, eta=eta, beta=2.0, early_stop_delay=0)\n",
    "# clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['val_loss_mse'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trained_model_results(df, net, x_train, y_train, y_train_days,  x_val, y_val, y_val_days, device, rul_or_percent='percent',\n",
    "                               loss_func='mse', early_stop_delay=0, save_pic=False, show_pic=True, save_name='results'):\n",
    "    fig, ax = plt.subplots(\n",
    "    2, 2, figsize=(14,9), \n",
    "    constrained_layout=True\n",
    "    )\n",
    "    \n",
    "    #### SELECTED LOSS FUNCTION\n",
    "    # in sample loss\n",
    "    ax[0][0].plot(df['epoch'], df['loss'], label='Loss',linewidth=2, alpha=0.4, color='#d73027')\n",
    "\n",
    "    # out of sample (validation) loss\n",
    "    ax[0][0].plot(df['epoch'], df['val_loss'], label='Val Loss',linewidth=2, color='#d73027')\n",
    "    \n",
    "    epoch_stopped_on = int(df[df['val_loss']==np.min(df['val_loss'][early_stop_delay:])]['epoch'])\n",
    "    ax[0][0].axvline(int(df[df['val_loss']==np.min(df['val_loss'][early_stop_delay:])]['epoch']), linestyle='--', color='black',label='Early Stopping Checkpoint', alpha=0.6)\n",
    "\n",
    "    ax[0][0].legend()\n",
    "    ax[0][0].set_xlabel('epoch')\n",
    "    ax[0][0].set_ylabel(f'{loss_func} loss')\n",
    "    ax[0][0].set_title(f'{loss_func} loss')\n",
    "    \n",
    "    #### MSE LOSS FUNCTION\n",
    "    # in sample loss\n",
    "    ax[0][1].plot(df['epoch'], df['loss'], label='Loss',linewidth=2, alpha=0.4, color='#d73027')\n",
    "\n",
    "    # out of sample (validation) loss\n",
    "    ax[0][1].plot(df['epoch'], df['val_loss_mse'], label='Val Loss',linewidth=2, color='#d73027')\n",
    "    ax[0][1].axvline(epoch_stopped_on, linestyle='--', color='black',label='Early Stopping Checkpoint', alpha=0.6)\n",
    "\n",
    "#     ax[0][1].legend(loc='lower right')\n",
    "    ax[0][1].set_xlabel('epoch')\n",
    "    ax[0][1].set_ylabel('mse loss')\n",
    "    ax[0][1].set_title('mse loss')\n",
    "    \n",
    "    mse_value = df['val_loss_mse'].iloc[epoch_stopped_on-1]\n",
    "    # get x and y axis limits\n",
    "    x_min, x_max = ax[0][1].get_xlim()\n",
    "    y_min, y_max = ax[0][1].get_ylim()\n",
    "    \n",
    "    print_text = f'MSE = {mse_value:.3f}'    \n",
    "    \n",
    "    ax[0][1].text((x_max-x_min)*0.5+x_min, y_max-(y_max-y_min)*0.1, print_text, fontsize='large', fontweight='semibold',\n",
    "                  verticalalignment='center', horizontalalignment='center',\n",
    "        bbox={'facecolor': 'gray', 'alpha': 0.2, 'pad': 10})\n",
    "    \n",
    "    print(df['epoch'].max()*0.1)\n",
    "    print(ax[0][1].get_ylim())\n",
    "\n",
    "    #### RUL/PERCENT CURVE\n",
    "    y_hats = test(net, x_train, 100)\n",
    "    \n",
    "    if rul_or_percent == 'rul':\n",
    "        index_sorted = np.flip(np.array(np.argsort(y_train, 0).reshape(-1)))\n",
    "        ax[1][0].plot(np.array(y_train)[index_sorted], label='True RUL')\n",
    "        ax[1][0].plot(y_hats[index_sorted], label='Predicted RUL')\n",
    "        ax[1][0].set_ylabel('RUL (days)')\n",
    "        ax[1][0].legend(loc='upper right')\n",
    "    \n",
    "    else:\n",
    "        index_sorted = np.array(np.argsort(y_train, 0).reshape(-1))\n",
    "        ax[1][0].plot(np.array(y_train)[index_sorted], label='True Life Percentage')\n",
    "        ax[1][0].plot(y_hats[index_sorted], label='Predicted Life Percentage')\n",
    "        ax[1][0].set_ylabel('life percentage')\n",
    "        ax[1][0].legend(loc='lower right')\n",
    "    \n",
    "    ax[1][0].set_title('train results')\n",
    "    \n",
    "    \n",
    "    y_hats = test(net, x_val, 100)\n",
    "    \n",
    "    if rul_or_percent == 'rul':\n",
    "        index_sorted = np.flip(np.array(np.argsort(y_val, 0).reshape(-1)))\n",
    "        ax[1][1].plot(np.array(y_val)[index_sorted], label='True RUL')\n",
    "        ax[1][1].plot(y_hats[index_sorted], label='Predicted RUL')\n",
    "        ax[1][1].set_ylabel('RUL (days)')\n",
    "        ax[1][1].legend(loc='upper right')\n",
    "    \n",
    "    else:\n",
    "        index_sorted = np.array(np.argsort(y_val, 0).reshape(-1))\n",
    "        ax[1][1].plot(np.array(y_val)[index_sorted], label='True Life Percentage')\n",
    "        ax[1][1].plot(y_hats[index_sorted], label='Predicted Life Percentage')\n",
    "        ax[1][1].set_ylabel('life percentage')\n",
    "        ax[1][1].legend(loc='lower right')\n",
    "    \n",
    "    ax[1][1].set_title('validation results')\n",
    "\n",
    "    if save_pic:\n",
    "        plt.savefig(f'{save_name}.png',format='png')\n",
    "    \n",
    "    if show_pic:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "plot_trained_model_results(df, net, x_train, y_train, y_train_days,  x_val, y_val, y_val_days, device,\n",
    "                               loss_func='weibull', save_pic=True, show_pic=True, save_name='results')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color blind colors, from https://bit.ly/3qJ6LYL\n",
    "# [#d73027, #fc8d59, #fee090, #4575b4]\n",
    "# [redish, orangeish, yellowish, blueish]\n",
    "# ['mse', 'physics_loss', approx_constraint_loss, 'combined']\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    1, 1, figsize=(14,9), \n",
    "#     dpi=150, constrained_layout=True\n",
    ")\n",
    "\n",
    "# in sample loss\n",
    "ax.plot(df['epoch'], df['loss'], label='Loss',linewidth=2, alpha=0.4, color='#d73027')\n",
    "\n",
    "# out of sample (validation) loss\n",
    "ax.plot(df['epoch'], df['val_loss'], label='Val Loss',linewidth=2, color='#d73027')\n",
    "plt.axvline(int(df[df['val_loss']==np.min(df['val_loss'][early_stop:])]['epoch']), linestyle='--', color='black',label='Early Stopping Checkpoint', alpha=0.6)\n",
    "\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Curves for Loss Function')\n",
    "# plt.savefig('mse_loss.png',format='png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hats = test(net, x_val, BATCH_SIZE)\n",
    "\n",
    "if rul_or_percent == 'rul':\n",
    "    index_sorted = np.flip(np.array(np.argsort(y_val, 0).reshape(-1)))\n",
    "else:\n",
    "    index_sorted = np.array(np.argsort(y_val, 0).reshape(-1))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,6),)\n",
    "\n",
    "\n",
    "if rul_or_percent == 'rul':\n",
    "    ax.plot(np.array(y_val)[index_sorted], label='True RUL')\n",
    "    ax.plot(y_hats[index_sorted], label='Predicted RUL')\n",
    "    ax.set_ylabel('RUL (days)')\n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "else:\n",
    "    ax.plot(np.array(y_val)[index_sorted], label='True Life Percentage')\n",
    "    ax.plot(y_hats[index_sorted], label='Predicted Life Percentage')\n",
    "    ax.set_ylabel('Life Percentage')\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "\n",
    "ax.set_xlabel('Sample (from earliest to latest)')\n",
    "plt.savefig('validation_set_results_set1.png',format='png')\n",
    "plt.show()\n",
    "\n",
    "# calculate percentage error\n",
    "error = torch.absolute(y_val - y_hats)/y_val*100\n",
    "\n",
    "# average percent error across all predictions\n",
    "error_avg = torch.mean(error[torch.isfinite(error)])\n",
    "print(f'Average error: {error_avg:.3f}')\n",
    "\n",
    "TSS = torch.sum((y_val - torch.mean(y_val))**2)\n",
    "\n",
    "RSS = torch.sum((torch.tensor(y_hats) - torch.mean(y_val))**2)\n",
    "\n",
    "r_squared = 1 - RSS/TSS\n",
    "print(f'R-squared: {r_squared:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2, y2 = create_x_y(df_spec2, labels_dict2,  bucket_size=bucket_size, print_shape=False)\n",
    "x2 = x2[1:] # get rid of the 'zero' so that we don't have inf in weibull loss func\n",
    "y2 = y2[1:]\n",
    "\n",
    "x_train = scaler(x2, min_val, max_val)\n",
    "x_train = torch.tensor(x_train).float()\n",
    "y_train = torch.tensor(y2).float()\n",
    "\n",
    "y_hats = test(net, x_train, BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "if rul_or_percent == 'rul':\n",
    "    y_train = torch.reshape(torch.Tensor(y_train[:,2]),(-1,1)).float()\n",
    "    index_sorted = np.flip(np.array(np.argsort(y_train, 0).reshape(-1)))\n",
    "else:\n",
    "    y_train = torch.reshape(torch.Tensor(y_train[:,1]),(-1,1)).float()\n",
    "    index_sorted = np.array(np.argsort(y_train, 0).reshape(-1))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,6),)\n",
    "\n",
    "\n",
    "if rul_or_percent == 'rul':\n",
    "    ax.plot(np.array(y_train)[index_sorted], label='True RUL')\n",
    "    ax.plot(y_hats[index_sorted], label='Predicted RUL')\n",
    "    ax.set_ylabel('RUL (days)')\n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "else:\n",
    "    ax.plot(np.array(y_train)[index_sorted], label='True Life Percentage')\n",
    "    ax.plot(y_hats[index_sorted], label='Predicted Life Percentage')\n",
    "    ax.set_ylabel('Life Percentage')\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "\n",
    "ax.set_xlabel('Sample (from earliest to latest)')\n",
    "plt.savefig('train_set_results_set2.png',format='png')\n",
    "plt.show()\n",
    "# calculate percentage error\n",
    "error = torch.absolute(y_train - y_hats)/y_train*100\n",
    "\n",
    "# average percent error across all predictions\n",
    "error_avg = torch.mean(error[torch.isfinite(error)])\n",
    "print(f'Average error: {error_avg:.3f}')\n",
    "\n",
    "TSS = torch.sum((y_train - torch.mean(y_train))**2)\n",
    "\n",
    "RSS = torch.sum((torch.tensor(y_hats) - torch.mean(y_train))**2)\n",
    "\n",
    "r_squared = 1 - RSS/TSS\n",
    "print(f'R-squared: {r_squared:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3, y3 = create_x_y(df_spec3, labels_dict3,  bucket_size=bucket_size, print_shape=False)\n",
    "x3 = x3[1:] # get rid of the 'zero' so that we don't have inf in weibull loss func\n",
    "y3 = y3[1:]\n",
    "\n",
    "x_train = scaler(x3, min_val, max_val)\n",
    "x_train = torch.tensor(x_train).float()\n",
    "y_train = torch.tensor(y3).float()\n",
    "\n",
    "y_hats = test(net, x_train, BATCH_SIZE)\n",
    "\n",
    "if rul_or_percent == 'rul':\n",
    "    y_train = torch.reshape(torch.Tensor(y_train[:,2]),(-1,1)).float()\n",
    "    index_sorted = np.flip(np.array(np.argsort(y_train, 0).reshape(-1)))\n",
    "else:\n",
    "    y_train = torch.reshape(torch.Tensor(y_train[:,1]),(-1,1)).float()\n",
    "    index_sorted = np.array(np.argsort(y_train, 0).reshape(-1))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,6),)\n",
    "\n",
    "\n",
    "if rul_or_percent == 'rul':\n",
    "    ax.plot(np.array(y_train)[index_sorted], label='True RUL')\n",
    "    ax.plot(y_hats[index_sorted], label='Predicted RUL')\n",
    "    ax.set_ylabel('RUL (days)')\n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "else:\n",
    "    ax.plot(np.array(y_train)[index_sorted], label='True Life Percentage')\n",
    "    ax.plot(y_hats[index_sorted], label='Predicted Life Percentage')\n",
    "    ax.set_ylabel('Life Percentage')\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "\n",
    "ax.set_xlabel('Sample (from earliest to latest)')\n",
    "plt.savefig('train_set_results_set3.png',format='png')\n",
    "plt.show()\n",
    "# calculate percentage error\n",
    "error = torch.absolute(y_train - y_hats)/y_train*100\n",
    "\n",
    "# average percent error across all predictions\n",
    "error_avg = torch.mean(error[torch.isfinite(error)])\n",
    "print(f'Average error: {error_avg:.3f}')\n",
    "\n",
    "TSS = torch.sum((y_train - torch.mean(y_train))**2)\n",
    "\n",
    "RSS = torch.sum((torch.tensor(y_hats) - torch.mean(y_train))**2)\n",
    "\n",
    "r_squared = 1 - RSS/TSS\n",
    "print(f'R-squared: {r_squared:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(x_train.shape[1])\n",
    "\n",
    "# setup to run on GPU (or CPU if GPU not avail.)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Running on GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on CPU\")\n",
    "    \n",
    "    \n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# SET TOTAL NUMBER OF EPOCHS FOR ALL EXPERIMENTS\n",
    "EPOCHS_ALL = 2000\n",
    "\n",
    "EPOCHS = EPOCHS_ALL\n",
    "\n",
    "net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# save the results in a dataframe\n",
    "df = train(net, x_train, y_train, y_train_days, x_test, y_test, y_test_days,  'rmsle', lambda_mod=3.0, eta=eta, beta=beta)\n",
    "# df = train(net, x_train, y_train, y_train_days, x_test, y_test, y_test_days,  'weibull', lambda_mod=3.0, eta=eta, beta=beta)\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train\n",
    "y_val = y_train\n",
    "\n",
    "y_hats = test(net, x_val, BATCH_SIZE)\n",
    "\n",
    "index_sorted = np.argsort(y_val, 0).reshape(-1)\n",
    "index_sorted.shape\n",
    "\n",
    "plt.plot(np.array(y_val)[index_sorted])\n",
    "plt.plot(y_hats[index_sorted])\n",
    "plt.show()\n",
    "\n",
    "# calculate percentage error\n",
    "error = torch.absolute(y_val - y_hats)/y_val*100\n",
    "\n",
    "# average percent error across all predictions\n",
    "error_avg = torch.mean(error[torch.isfinite(error)])\n",
    "print(f'Average error: {error_avg:.3f}')\n",
    "\n",
    "TSS = torch.sum((y_val - torch.mean(y_val))**2)\n",
    "\n",
    "RSS = torch.sum((torch.tensor(y_hats) - torch.mean(y_val))**2)\n",
    "\n",
    "r_squared = 1 - RSS/TSS\n",
    "print(f'R-squared: {r_squared:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - RSS/TSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = y_hats[index_sorted][1:][:,0]\n",
    "real_val = np.array(y_test)[index_sorted][1:][:,0]\n",
    "print(pred_val.shape)\n",
    "print(real_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.absolute(real_val-pred_val)/real_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(real_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.absolute(np.array(y_test)[index_sorted][1:] - y_hats[index_sorted][1:])/(np.array(y_test)[index_sorted][1:])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hats[index_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(error[torch.isfinite(error)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (torch.absolute(y_test - y_hats)/y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(32 - (32-33))/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.absolute(32 -31)/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hats[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(a[0:23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, d =torch.where(a<0)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(acc[torch.isfinite(acc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(acc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weibull 20 input size, lambda=3\n",
    "y_hats = test(net, x_train, BATCH_SIZE)\n",
    "\n",
    "index_sorted = np.argsort(y_train, 0).reshape(-1)\n",
    "index_sorted.shape\n",
    "\n",
    "plt.plot(np.array(y_train)[index_sorted])\n",
    "plt.plot(y_hats[index_sorted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weibull 20 input size, lambda=1\n",
    "y_hats = test(net, x_test, BATCH_SIZE)\n",
    "\n",
    "index_sorted = np.argsort(y_test, 0).reshape(-1)\n",
    "index_sorted.shape\n",
    "\n",
    "plt.plot(np.array(y_test)[index_sorted])\n",
    "plt.plot(y_hats[index_sorted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weibull 20 input size, lambda=1\n",
    "y_hats = test(net, x_train, BATCH_SIZE)\n",
    "\n",
    "index_sorted = np.argsort(y_train, 0).reshape(-1)\n",
    "index_sorted.shape\n",
    "\n",
    "plt.plot(np.array(y_train)[index_sorted])\n",
    "plt.plot(y_hats[index_sorted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSLE 20 input size\n",
    "plt.plot(np.array(y_train)[index_sorted])\n",
    "plt.plot(y_hats[index_sorted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSLE\n",
    "plt.plot(np.array(y_test)[index_sorted])\n",
    "plt.plot(y_hats[index_sorted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE\n",
    "plt.plot(np.array(y_test)[index_sorted])\n",
    "plt.plot(y_hats[index_sorted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE\n",
    "plt.plot(np.array(y_test)[index_sorted])\n",
    "plt.plot(y_hats[index_sorted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "\n",
    "# setup to run on GPU (or CPU if GPU not avail.)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Running on GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on CPU\")\n",
    "    \n",
    "    \n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# SET TOTAL NUMBER OF EPOCHS FOR ALL EXPERIMENTS\n",
    "EPOCHS_ALL = 1000\n",
    "\n",
    "EPOCHS = EPOCHS_ALL\n",
    "\n",
    "net = Net()\n",
    "net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# save the results in a dataframe\n",
    "df = train(net, x_train, y_train, y_train_days, x_test, y_test, y_test_days,  'weibull', lambda_mod=1.0, eta=eta, beta=beta)\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_sorted = np.argsort(y_test, 0).reshape(-1)\n",
    "index_sorted.shape\n",
    "\n",
    "plt.plot(np.array(y_test)[index_sorted])\n",
    "plt.plot(y_hats[index_sorted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(y_test)[index_sorted])\n",
    "plt.plot(y_hats[index_sorted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(y_test)[index_sorted])\n",
    "plt.plot(y_hats[index_sorted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(\n",
    "    1, 1, figsize=(14,9), \n",
    "#     dpi=150, constrained_layout=True\n",
    ")\n",
    "\n",
    "ax.plot(x, y[:,1], color='grey',label='True Signal\\n(no noise)', linewidth=5, alpha=0.3)\n",
    "# ax.scatter(x_test, y_hats, s=7, label='Predictions', color='#d73027', alpha=0.7 )\n",
    "# ax.scatter(x_train, y_train_noisy, s=20, alpha=0.8, label='Training Data',linewidths=0, marker='D')\n",
    "ax.legend(loc='lower left')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Predictions using MSE Loss Function')\n",
    "# plt.savefig('mse_predictions.png',format='png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pdf_cdf](pdf_cdf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull(t, eta, beta):\n",
    "    \"weibull PDF function\"\n",
    "    return (beta/(eta ** beta))*(t**(beta-1.0))*np.exp(-1.0*((t/eta)**beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1000.0\n",
    "beta = 5.0\n",
    "c = eta\n",
    "k = beta\n",
    "\n",
    "t = np.linspace(0,2000,500)\n",
    "\n",
    "f = weibull(t, c, k)\n",
    "plt.plot(t, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Weiubull Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_size = 400\n",
    "\n",
    "df_temp = df_spec.iloc[:10000]\n",
    "a = np.array(df_temp) # make numpy array\n",
    "\n",
    "# get the y-axis (frequency values)\n",
    "y = np.array(df_temp.index)\n",
    "y = np.max(y.reshape(-1,bucket_size),axis=1)\n",
    "\n",
    "# get the max value for each bucket\n",
    "# https://stackoverflow.com/a/15956341/9214620\n",
    "max_a = np.max(a.reshape(-1,bucket_size,2156),axis=1)\n",
    "\n",
    "print('shape of max_a array:', np.shape(max_a))\n",
    "\n",
    "# get the mean value for each bucket\n",
    "avg_a = np.mean(a.reshape(-1,bucket_size,2156),axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "plt.pcolormesh(max_a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(max_a[10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 10th bucket the x value\n",
    "x = []\n",
    "for t in labels_dict.keys():\n",
    "    x.append(labels_dict[t][-1])\n",
    "\n",
    "x = np.sort(np.array(x))\n",
    "# x = np.sort(np.array(x)[::20])\n",
    "print(np.shape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = max_a[10]\n",
    "# y = max_a[10][::20]\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y)\n",
    "plt.ylabel(\"Acceleration\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential(x, a, k, b):\n",
    "    return a*np.exp(x*k) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popt_exponential, pcov_exponential = curve_fit(exponential, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, exponential(x, *popt_exponential))\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.unm.edu/~mr369/python/data-analysis.html\n",
    "def weibull(x, c, k, a):\n",
    "    \"weibull to fit\"\n",
    "    return a+(k/c)*((x/c)**(k-1.0))*np.exp(-1.0*((x/c)**k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3 * 10 ** 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popt, pcov = curve_fit(weibull, x, y,bounds=(0, [100.0, 30.0, 0.2]))\n",
    "popt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y)\n",
    "plt.plot(x, weibull(x, *popt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ufrf(x, n, b, Y, K):\n",
    "    \"universal failure rate function to fit\"\n",
    "    return Y + K * (b/(n ** b))*x**(b-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale between 0 and 1\n",
    "y = (y - np.min(y))/np.max(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popt, pcov = curve_fit(ufrf, x, y,\n",
    "                       bounds=(0.0001, [100.0, 20, 10.0, 10.0])\n",
    "                      )\n",
    "popt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y)\n",
    "plt.plot(x, ufrf(x, *popt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h(t) = (\\beta / \\eta)(t/ \\eta)^{\\beta - 1}$ - Weibull hazard function (from 2-7 in The New Weibull Handbook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_hazard(t, n, b):\n",
    "    return (b / n) * (t / n) ** (b -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y)\n",
    "plt.plot(x, weibull_hazard(x, 38, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, 1-np.exp(-x/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta, n, Y, K = popt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = Y + K * (beta)/(np.power(n, beta)) * np.power(x, (beta - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta, n, Y, K = (5.0, 100.0, 1.9769, 0.000355)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = func(x, beta, n, Y, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func(x, *popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, func(x, *popt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/3433486/how-to-do-exponential-and-logarithmic-curve-fitting-in-python-i-found-only-poly\n",
    "a = np.polyfit(x,np.log(y),1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y)\n",
    "plt.plot(x, np.exp(a[0])*np.exp(a[1]*x))\n",
    "plt.ylabel(\"Acceleration\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([10, 19, 30, 35, 51])\n",
    "y = np.array([1, 7, 20, 50, 79])\n",
    "\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " y = AeBx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "plt.pcolormesh(avg_a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Spectrogram in Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Heatmap(\n",
    "        z=df_spec,\n",
    "        x=date_list,\n",
    "        y=df_spec.index,\n",
    "        colorscale='Viridis'))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_nticks=2)\n",
    "\n",
    "pio.write_html(fig,file='test.html',auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = sorted(os.listdir(folder_1st))\n",
    "date_list = date_list[int((1-len(date_list)*0.2/len(date_list))*len(date_list)):] # get the last 20% of dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_1st = folder_raw_data / '1st_test'\n",
    "\n",
    "# reminder of bearing acceleration channels\n",
    "# col_names = ['b1_ch1', 'b1_ch2', 'b2_ch3', 'b2_ch4', 'b3_ch5', 'b3_ch6', 'b4_ch7', 'b4_ch8']\n",
    "\n",
    "df_spec, labels_dict = build_spectrogram_df(folder_1st, date_list, channel_name='b3_ch6', start_time=date_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spec = df_spec * 10**6\n",
    "df_spec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spec = df_spec.astype(dtype='int')\n",
    "df_spec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = sorted(os.listdir(folder_1st))\n",
    "date_list = date_list[int((1-len(date_list)*0.2/len(date_list))*len(date_list)):] # get the last 20% of dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "        z=df_spec,\n",
    "        x=df_spec.columns,\n",
    "        y=df_spec.index,\n",
    "        colorscale='Viridis'))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_nticks=2)\n",
    "\n",
    "pio.write_html(fig,file='test.html',auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import datetime\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "programmers = ['Alex','Nicole','Sara','Etienne','Chelsea','Jody','Marianne']\n",
    "\n",
    "base = datetime.datetime.today()\n",
    "dates = base - np.arange(180) * datetime.timedelta(days=1)\n",
    "z = np.random.poisson(size=(len(programmers), len(dates)))\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "        z=z,\n",
    "        x=dates,\n",
    "        y=programmers,\n",
    "        colorscale='Viridis'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='GitHub commits per day',\n",
    "    xaxis_nticks=36)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rolling average\n",
    "# df_spec = df_spec.rolling(25,axis=1).mean()\n",
    "# df_spec = df_spec.dropna(axis=1)\n",
    "# df_spec.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
