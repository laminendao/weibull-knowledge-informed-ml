{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from src.models.model import Net\n",
    "\n",
    "from src.data.data_utils import load_train_test_ims, load_train_test_femto\n",
    "from src.models.utils import test, calc_r2_avg, model_metrics_test, test_metrics_to_results_df\n",
    "from src.models.loss import RMSELoss, RMSLELoss\n",
    "import src.models.model\n",
    "# from visualizations import plot_trained_model_results\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "\n",
    "import fnmatch\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the folder location of the temporary results\n",
    "root_dir = Path.cwd().parent\n",
    "print(root_dir)\n",
    "temp_dir = root_dir / 'models/interim/results_csv_ims'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that the model.py is in the current notebook directory\n",
    "# so that we can load the checkpoints\n",
    "if Path(Path.cwd() / 'model.py').exists():\n",
    "    pass\n",
    "else:\n",
    "    shutil.copy(root_dir / 'src/models/model.py', Path.cwd() / 'model.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use multi-processing to load all the CSVs into one file\n",
    "# https://stackoverflow.com/a/36590187\n",
    "# wrap your csv importer in a function that can be mapped\n",
    "def read_csv(filename):\n",
    "    'converts a filename to a pandas dataframe'\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "\n",
    "def main(folder_path):\n",
    "\n",
    "    # get a list of file names\n",
    "    files = os.listdir(folder_path)\n",
    "    file_list = [temp_dir / filename for filename in files if filename.endswith('.csv')]\n",
    "\n",
    "    # set up your pool\n",
    "    with Pool(processes=7) as pool: # or whatever your hardware can support\n",
    "\n",
    "        # have your pool map the file names to dataframes\n",
    "        df_list = pool.map(read_csv, file_list)\n",
    "\n",
    "        # reduce the list of dataframes to a single dataframe\n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "        \n",
    "        return combined_df\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = main(temp_dir)\n",
    "    \n",
    "# drop first column\n",
    "try:\n",
    "    df = df.drop(columns='Unnamed: 0')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# export combined dataframe\n",
    "# csv sav_name\n",
    "csv_save_name = 'combined_results_2021.04.05_1.csv'\n",
    "\n",
    "# add a unique identifier for each model architecture\n",
    "df['date_time_seed'] = df['date_time'].astype(str)+'_'+df['rnd_seed_input'].astype(str)\n",
    "\n",
    "# get name that model checkpoint was saved under\n",
    "df['model_checkpoint_name'] = df['date_time'].astype(str)+'_'+df['loss_func'] +'_'+df['rnd_seed_input'].astype(str)+'.pt'\n",
    "\n",
    "# move 'date_time_seed' to front\n",
    "# df = df[[list(df).pop()] + list(df)[:-1]]\n",
    "\n",
    "df.to_csv(csv_save_name, index=False)\n",
    "print('Final df shape:',df.shape)\n",
    "\n",
    "#### append test results to df ####\n",
    "ADD_TEST_RESULTS = True # True or False\n",
    "\n",
    "if ADD_TEST_RESULTS:\n",
    "    folder_path = root_dir / 'data/processed/IMS/'\n",
    "\n",
    "    (\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        x_train_2,\n",
    "        y_train_2,\n",
    "        x_train_3,\n",
    "        y_train_3,\n",
    "    ) = load_train_test_ims(folder_path)\n",
    "    \n",
    "    \n",
    "#     (\n",
    "#         x_train,\n",
    "#         y_train,\n",
    "#         x_val,\n",
    "#         y_val,\n",
    "#         x_test,\n",
    "#         y_test,\n",
    "#         x_train1_1,\n",
    "#         y_train1_1,\n",
    "#         x_train2_1,\n",
    "#         y_train2_1,\n",
    "#         x_train3_1,\n",
    "#         y_train3_1,\n",
    "#         x_val1_2,\n",
    "#         y_val1_2,\n",
    "#         x_val2_2,\n",
    "#         y_val2_2,\n",
    "#         x_val3_2,\n",
    "#         y_val3_2,\n",
    "#         x_test1_3,\n",
    "#         y_test1_3,\n",
    "#         x_test2_3,\n",
    "#         y_test2_3,\n",
    "#         x_test3_3,\n",
    "#         y_test3_3,\n",
    "#     ) = load_train_test_femto(folder_path)\n",
    "\n",
    "\n",
    "    # load beta, eta for Weibull CDF\n",
    "    with h5py.File(folder_path / \"eta_beta_r.hdf5\", \"r\") as f:\n",
    "        eta_beta_r = f[\"eta_beta_r\"][:]\n",
    "\n",
    "    ETA = eta_beta_r[0]\n",
    "    BETA = eta_beta_r[1]\n",
    "\n",
    "    y_train_days = torch.reshape(y_train[:, 0], (-1, 1))\n",
    "    y_val_days = torch.reshape(y_val[:, 0], (-1, 1))\n",
    "    y_test_days = torch.reshape(y_test[:, 0], (-1, 1))\n",
    "\n",
    "    y_train_days_2 = torch.reshape(y_train_2[:, 0], (-1, 1))\n",
    "    y_train_days_3 = torch.reshape(y_train_3[:, 0], (-1, 1))\n",
    "\n",
    "\n",
    "    y_train = torch.reshape(y_train[:, 1], (-1, 1))\n",
    "    y_val = torch.reshape(y_val[:, 1], (-1, 1))\n",
    "    y_test = torch.reshape(y_test[:, 1], (-1, 1))\n",
    "\n",
    "    y_train_2 = torch.reshape(y_train_2[:, 1], (-1, 1))\n",
    "    y_train_3 = torch.reshape(y_train_3[:, 1], (-1, 1))\n",
    "    \n",
    "    model_folder = root_dir / 'models/interim/checkpoints_ims/'\n",
    "    print(model_folder)\n",
    "\n",
    "    # append test results onto results dataframe\n",
    "    df_results = test_metrics_to_results_df(model_folder, df, x_test, y_test)\n",
    "    \n",
    "    standard_losses = ['mse', 'rmse', 'rmsle']\n",
    "\n",
    "    # apply 0 or 1 for weibull, and for each unique loss func\n",
    "    for index, value in df_results['loss_func'].items():\n",
    "        if value in standard_losses:\n",
    "            df_results.loc[index, 'weibull_loss'] = 0\n",
    "        else:\n",
    "            df_results.loc[index, 'weibull_loss'] = 1\n",
    "\n",
    "    # convert to 'weibull_loss' column to integer\n",
    "    df_results['weibull_loss'] = df_results['weibull_loss'].astype(int)\n",
    "\n",
    "\n",
    "    loss_func_list = df_results['loss_func'].unique()\n",
    "\n",
    "    for index, value in df_results['loss_func'].items():\n",
    "        for loss_func in loss_func_list:\n",
    "            df_results.loc[index, value] = 1\n",
    "\n",
    "    df_results[loss_func_list] = df_results[loss_func_list].fillna(0, downcast='infer')\n",
    "    \n",
    "    \n",
    "    df_results.to_csv('combined_results_2021.04.05_1_with_test.csv', index=False)\n",
    "    \n",
    "clear_output(wait=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the metrics used to filter the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Results and Look at Top Performing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the results by $R^2$, RMSE, and select top performing models from each batch of models. Chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv\n",
    "csv_save_name = 'combined_results_2021.04.05_1_with_test.csv'\n",
    "\n",
    "# test adding the 0 or 1 for whether standard or Weibull loss functions\n",
    "dfr = pd.read_csv(csv_save_name)\n",
    "\n",
    "# how many unique model architectures?\n",
    "print('No. unique model architectures:', len(dfr['date_time_seed'].unique()))\n",
    "print('No. unique models (includes unique loss functions):', len(dfr['date_time_seed']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv\n",
    "csv_save_name = 'combined_results_2021.04.05_1_with_test.csv'\n",
    "\n",
    "\n",
    "# test adding the 0 or 1 for whether standard or Weibull loss functions\n",
    "dfr = pd.read_csv(csv_save_name)\n",
    "\n",
    "print('original dfr shape:', dfr.shape)\n",
    "\n",
    "loss_func_list = dfr['loss_func'].unique()\n",
    "# add filter\n",
    "# tight filter\n",
    "# r2_bound = 0.3 # greater than\n",
    "# rmse_bound = 0.25 # less than\n",
    "\n",
    "# looser filter\n",
    "r2_bound = 0.2 # greater than\n",
    "rmse_bound = 0.35 # less than\n",
    "\n",
    "# even looser filter\n",
    "# r2_bound = 0.0 # greater than\n",
    "# rmse_bound = 1.0 # less than\n",
    "\n",
    "# define what metric we'll use to evaulatate results\n",
    "# sort_by = 'loss_rmse_test'\n",
    "# sort_by = 'r2_val'\n",
    "sort_by = 'r2_test'\n",
    "# sort_by = 'r2_test_avg'\n",
    "\n",
    "dfr = dfr[(dfr['r2_test']>r2_bound) & \n",
    "         (dfr['loss_rmse_test']<rmse_bound) &\n",
    "         (dfr['r2_train']>r2_bound) &\n",
    "         (dfr['loss_rmse_train']<rmse_bound) &\n",
    "         (dfr['r2_val']>r2_bound) & \n",
    "         (dfr['loss_rmse_val']<rmse_bound) &\n",
    "         (dfr['beta']==2.0)\n",
    "][:]\n",
    "\n",
    "print('shape of dfr before selecting top models in each architecture:', dfr.shape)\n",
    "\n",
    "dfr = dfr.groupby(['date_time_seed']).apply(lambda x: x.sort_values([sort_by], ascending = False)).reset_index(drop=True)\n",
    "dfr = dfr.groupby(['date_time_seed']).head(1).sort_values(by=sort_by, ascending=False)\n",
    "# dfr = dfr.groupby(['date_time_seed', 'weibull_loss']).head(1).sort_values(by=sort_by, ascending=False)\n",
    "\n",
    "# drop group if any models in the group did not train\n",
    "# https://stackoverflow.com/a/34715183/9214620\n",
    "# dfr = dfr.groupby(['date_time_seed']).filter(lambda x: x[sort_by].min() > 0.2)\n",
    "dfr.to_csv('best_results_ims.csv', index=False)\n",
    "print(dfr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\",palette='bright')\n",
    "ax = sns.boxplot(x=dfr['weibull_loss'], y=dfr[sort_by])\n",
    "ax = sns.regplot(x=dfr['weibull_loss'], y=dfr[sort_by], scatter=False)\n",
    "plt.show()\n",
    "\n",
    "weibull_or_not = dfr['weibull_loss']\n",
    "r2_test = dfr[sort_by]\n",
    "\n",
    "pbc = pointbiserialr(weibull_or_not, r2_test)\n",
    "print(pbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_loss_func_name(cols):\n",
    "    loss_func = cols[0]\n",
    "    \n",
    "    if loss_func == 'mse':\n",
    "        return 'MSE'\n",
    "    elif loss_func == 'rmse':\n",
    "        return 'RMSE'\n",
    "    elif loss_func == 'rmsle':\n",
    "        return 'RMSLE'\n",
    "    elif loss_func == 'weibull_mse':\n",
    "        return 'Weibull-MSE\\nCombined'\n",
    "    elif loss_func == 'weibull_rmse':\n",
    "        return 'Weibull-RMSE\\nCombined'\n",
    "    elif loss_func == 'weibull_rmsle':\n",
    "        return 'Weibull-RMSLE\\nCombined'\n",
    "    elif loss_func == 'weibull_only_mse':\n",
    "        return 'Weibull Only MSE'\n",
    "    elif loss_func == 'weibull_only_rmse':\n",
    "        return 'Weibull Only RMSE'\n",
    "    else:\n",
    "        return 'Weibull Only RMLSE'    \n",
    "\n",
    "df_count = dfr.groupby(['loss_func'], as_index=False).count()[['loss_func', 'date_time']].rename(columns={'date_time':'count'}).sort_values(by='count',ascending=False)\n",
    "df_count['loss_func2'] = df_count[['loss_func']].apply(change_loss_func_name, axis=1)\n",
    "df_count = df_count.drop('loss_func', axis=1)\n",
    "df_count = df_count.rename(columns={'loss_func2':'loss_func'})\n",
    "df_count['count'] = df_count['count'].astype(float)\n",
    "df_count['percent'] = 100 * df_count['count'] / df_count['count'].sum()\n",
    "\n",
    "df_count.to_csv('ims_count_results.csv', index=False)\n",
    "df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(7,7),)\n",
    "\n",
    "sns.set(font_scale=1.1, style=\"whitegrid\")\n",
    "ax = sns.barplot(\n",
    "    x=\"percent\", y=\"loss_func\", data=df_count, palette=\"Blues_d\",\n",
    ")\n",
    "for p in ax.patches:\n",
    "    # help from https://stackoverflow.com/a/56780852/9214620\n",
    "    space = 0.5\n",
    "    _x = p.get_x() + p.get_width() + float(space)\n",
    "    _y = p.get_y() + p.get_height() / 2\n",
    "    value = p.get_width()\n",
    "    ax.text(_x, _y, f\"{value:.1f} %\", ha=\"left\", va=\"center\", weight=\"semibold\", size=12)\n",
    "\n",
    "ax.spines[\"bottom\"].set_visible(True)\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.grid(alpha=0.7, linewidth=1, axis=\"x\")\n",
    "ax.set_xticks([0])\n",
    "ax.set_xticklabels([])\n",
    "plt.title(\"Most Successful Loss Function, by Count\", loc=\"left\")\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "# plt.savefig('loss_func_count_femto.png',dpi=600,bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart showing both IMS and PRONOSTIA percentages\n",
    "# load dataframes\n",
    "ims_folder = Path('/home/tim/Documents/bearing/notebooks/results_complete/2021.04.05_results_ims')\n",
    "pron_folder = Path('/home/tim/Documents/bearing/notebooks/results_complete/2021.04.07_results_femto')\n",
    "\n",
    "ims_save_name = 'ims_count_results.csv'\n",
    "pron_save_name = 'pronostia_count_results.csv'\n",
    "\n",
    "# test adding the 0 or 1 for whether standard or Weibull loss functions\n",
    "dfi = pd.read_csv(ims_folder / ims_save_name)\n",
    "dfp = pd.read_csv(pron_folder / pron_save_name)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14,7),)\n",
    "\n",
    "\n",
    "title_list = [r\"$\\bf{(a)}$\"+\" IMS Most Successful Loss Function, by Percentage\", \n",
    "              r\"$\\bf{(b)}$\"+' PRONOSTIA Most Successful Loss Function, by Percentage']\n",
    "df_list = [dfi, dfp]\n",
    "\n",
    "sns.set(style=\"whitegrid\", font=\"DejaVu Sans\")\n",
    "font_size = 14\n",
    "\n",
    "\n",
    "for ax, df, title in zip(axes.flat, df_list, title_list):\n",
    "\n",
    "    ax = sns.barplot(\n",
    "        x=\"percent\", y=\"loss_func\", data=df, palette=\"Blues_d\", ax=ax\n",
    "    )\n",
    "\n",
    "    for p in ax.patches:\n",
    "        # help from https://stackoverflow.com/a/56780852/9214620\n",
    "        space = 0.5\n",
    "        _x = p.get_x() + p.get_width() + float(space)\n",
    "        _y = p.get_y() + p.get_height() / 2\n",
    "        value = p.get_width()\n",
    "        ax.text(_x, _y, f\"{value:.1f} %\", ha=\"left\", va=\"center\", weight=\"semibold\", size=font_size)\n",
    "\n",
    "    ax.spines[\"bottom\"].set_visible(True)\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.grid(alpha=0.7, linewidth=1, axis=\"x\")\n",
    "    ax.set_xticks([0])\n",
    "    ax.set_xticklabels([])\n",
    "#     ax.text(-0.4, 1.06, title,\n",
    "#     verticalalignment='top', horizontalalignment='left',\n",
    "#     transform=ax.transAxes,\n",
    "#     color='black', fontsize=12)\n",
    "    \n",
    "    ax.set_title(title, fontsize=font_size, loc='right')\n",
    "    ax.tick_params(axis='y', labelsize=font_size)\n",
    "    \n",
    "\n",
    "plt.subplots_adjust(wspace=0.8)\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.savefig('loss_function_percentages.pdf',format='pdf', dpi=300, bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path(\"temp_pics/\").mkdir(parents=True, exist_ok=True)\n",
    "# cwd = Path.cwd()\n",
    "# temp_path = Path(\"temp_pics/\")\n",
    "\n",
    "# for i, row in dfr.iterrows():\n",
    "#     date_time = str(row['date_time'])\n",
    "#     rnd_seed_input = str(row['rnd_seed_input']) \n",
    "# #     loss_func = str(row['loss_func'])\n",
    "    \n",
    "#     for file in os.listdir('learning_curves/'):\n",
    "# #         if fnmatch.fnmatch(file, f'{date_time}_{loss_func}_{rnd_seed_input}.png'):\n",
    "#         if fnmatch.fnmatch(file, f'{date_time}*{rnd_seed_input}.png'):\n",
    "# #             print(file)\n",
    "            \n",
    "#             shutil.copy(cwd / 'learning_curves' / file, temp_path / file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Results and Look Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv\n",
    "csv_save_name = 'combined_results_2021.04.05_1_with_test.csv'\n",
    "\n",
    "# test adding the 0 or 1 for whether standard or Weibull loss functions\n",
    "dfr = pd.read_csv(csv_save_name)\n",
    "\n",
    "# add filter\n",
    "# tight filter\n",
    "# r2_bound = 0.3 # greater than\n",
    "# rmse_bound = 0.25 # less than\n",
    "\n",
    "# looser filter\n",
    "r2_bound = 0.2 # greater than\n",
    "rmse_bound = 0.35 # less than\n",
    "\n",
    "# even looser filter\n",
    "# r2_bound = 0.0 # greater than\n",
    "# rmse_bound = 1.0 # less than\n",
    "\n",
    "# define what metric we'll use to evaulatate results\n",
    "# sort_by = 'loss_rmse_test'\n",
    "# sort_by = 'r2_val'\n",
    "sort_by = 'r2_test'\n",
    "# sort_by = 'r2_test_avg'\n",
    "\n",
    "\n",
    "# dfr = dfr.groupby(['date_time_seed']).apply(lambda x: x.sort_values([sort_by], ascending = False)).reset_index(drop=True)\n",
    "# dfr = dfr.groupby(['date_time_seed']).head(1).sort_values(by=sort_by, ascending=False)\n",
    "# dfr = dfr.groupby(['date_time_seed', 'weibull_loss']).head(1).sort_values(by=sort_by, ascending=False)\n",
    "\n",
    "dfr = dfr[(dfr['r2_test']>r2_bound) & \n",
    "         (dfr['loss_rmse_test']<rmse_bound) &\n",
    "         (dfr['r2_train']>r2_bound) &\n",
    "         (dfr['loss_rmse_train']<rmse_bound) &\n",
    "         (dfr['r2_val']>r2_bound) & \n",
    "         (dfr['loss_rmse_val']<rmse_bound) &\n",
    "         (dfr['beta']==2.0)\n",
    "][:]\n",
    "\n",
    "# drop group if any models in the group did not train\n",
    "# https://stackoverflow.com/a/34715183/9214620\n",
    "# dfr = dfr.groupby(['date_time_seed']).filter(lambda x: x[sort_by].min() > 0.2)\n",
    "# dfr.to_csv('rmse_best_results.csv', index=False)\n",
    "print(dfr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_loss_func_name(cols):\n",
    "    loss_func = cols[0]\n",
    "    \n",
    "    if loss_func == 'mse':\n",
    "        return 'MSE'\n",
    "    elif loss_func == 'rmse':\n",
    "        return 'RMSE'\n",
    "    elif loss_func == 'rmsle':\n",
    "        return 'RMSLE'\n",
    "    elif loss_func == 'weibull_mse':\n",
    "        return 'Weibull-MSE\\nCombined'\n",
    "    elif loss_func == 'weibull_rmse':\n",
    "        return 'Weibull-RMSE\\nCombined'\n",
    "    elif loss_func == 'weibull_rmsle':\n",
    "        return 'Weibull-RMSLE\\nCombined'\n",
    "    elif loss_func == 'weibull_only_mse':\n",
    "        return 'Weibull Only\\nMSE'\n",
    "    elif loss_func == 'weibull_only_rmse':\n",
    "        return 'Weibull Only\\nRMSE'\n",
    "    else:\n",
    "        return 'Weibull Only\\nRMLSE'    \n",
    "\n",
    "df_c = dfr[list(loss_func_list) + [sort_by]].copy()\n",
    "\n",
    "results = {}\n",
    "for i in loss_func_list:\n",
    "    results[i] = list(pointbiserialr(df_c[i], df_c[sort_by]))\n",
    "    \n",
    "df_r = pd.DataFrame.from_dict(results).T\n",
    "df_r = df_r.rename(columns={0: 'corr', 1:'p_value'}).sort_values(by='corr').sort_values(by='corr',ascending=False)\n",
    "df_r['loss_func2'] = df_r.index # reset index\n",
    "df_r = df_r.reset_index(drop=True)\n",
    "df_r['loss_func'] = df_r[['loss_func2']].apply(change_loss_func_name, axis=1)\n",
    "df_r = df_r.drop('loss_func2', axis=1)\n",
    "df_r.to_csv('ims_correlation_results.csv', index=False)\n",
    "df_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_by = 'r2_test'\n",
    "\n",
    "\n",
    "sns.set(style=\"ticks\",palette='bright')\n",
    "ax = sns.boxplot(x=dfr['weibull_rmsle'], y=dfr[sort_by], color='white')\n",
    "\n",
    "\n",
    "for i,box in enumerate(ax.artists):\n",
    "    box.set_edgecolor('black')\n",
    "    box.set_facecolor('white')\n",
    "\n",
    "    # iterate over whiskers and median lines\n",
    "    for j in range(6*i,6*(i+1)):\n",
    "         ax.lines[j].set_color('black')\n",
    "\n",
    "ax = sns.regplot(x=dfr['weibull_rmsle'], y=dfr[sort_by], scatter=False)\n",
    "            \n",
    "plt.ylim(bottom=0.5)\n",
    "sns.despine(offset=5, trim=True)\n",
    "plt.show()\n",
    "\n",
    "weibull_or_not = dfr['weibull_rmsle']\n",
    "r2_test = dfr[sort_by]\n",
    "\n",
    "pbc = pointbiserialr(weibull_or_not, r2_test)\n",
    "print(pbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r = df_r.dropna(axis=0)\n",
    "sns.set(font_scale=1.1, style=\"whitegrid\")\n",
    "f, ax = plt.subplots(1,1, figsize=(8,4))\n",
    "ax = sns.barplot(x='loss_func',y='corr',ax=ax, palette='rocket', data=df_r)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90,)\n",
    "\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "# ax.axes.get_yaxis().set_visible(False) # hide y-axis\n",
    "ax.grid(alpha=0.7, linewidth=1, axis=\"y\")\n",
    "ax.xaxis.set_label_text(\"\",  size='large', weight='semibold')\n",
    "ax.yaxis.set_label_text(\"\", size='large', weight='semibold')\n",
    "ax.set_yticks([0])\n",
    "ax.set_yticklabels([], alpha=0)\n",
    "ax.set_xticklabels([], alpha=0)\n",
    "# plt.rc('xtick', labelsize='medium') \n",
    "# plt.rc('font', weight='semibold') \n",
    "\n",
    "for i, p in enumerate(ax.patches):\n",
    "    # help from https://stackoverflow.com/a/56780852/9214620\n",
    "    space = np.absolute(df_r['corr'].max()*0.1)\n",
    "\n",
    "    value = p.get_height()\n",
    "    if value >=0:\n",
    "        _y = p.get_y() + p.get_height() + float(space)\n",
    "        _x = p.get_x() + p.get_width() / 2\n",
    "        ax.text(_x, _y, f\"{value:.2f}\", ha=\"center\", va=\"center\", weight=\"semibold\", size=14)\n",
    "#         ax.text(_x, -space, df_r['loss_func'][i], ha=\"center\", va=\"top\", weight=\"normal\",multialignment='right', size=12, \n",
    "#                 rotation='vertical'\n",
    "#                )\n",
    "        ax.text(_x+0.2, -0.01, df_r['loss_func'][i], ha=\"right\", va=\"top\", weight=\"normal\",multialignment='right', size=14, \n",
    "                rotation=65\n",
    "               )\n",
    "\n",
    "    else:\n",
    "        _y = p.get_y() + p.get_height() - float(space)\n",
    "        _x = p.get_x() + p.get_width() / 2\n",
    "        ax.text(_x, _y, f\"{value:.2f}\", ha=\"center\", va=\"center\", weight=\"semibold\", size=14)\n",
    "        ax.text(_x-0.2, 0.01, df_r['loss_func'][i], ha=\"left\", va=\"bottom\", weight=\"normal\",multialignment='left', size=14, \n",
    "                rotation=65\n",
    "               )\n",
    "\n",
    "\n",
    "plt.rcParams['axes.titlepad'] = 20\n",
    "plt.title(\"Correlation of Loss Function with Test $R^2$\", loc=\"left\")\n",
    "# plt.xlim(0,1)\n",
    "# plt.ylim(-0.5,3.5)\n",
    "plt.savefig('correlation_ims.png',dpi=300, bbox_inches = \"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart showing both IMS and PRONOSTIA percentages\n",
    "# load dataframes\n",
    "ims_folder = Path('/home/tim/Documents/bearing/notebooks/results_complete/2021.04.05_results_ims')\n",
    "pron_folder = Path('/home/tim/Documents/bearing/notebooks/results_complete/2021.04.07_results_femto')\n",
    "\n",
    "ims_save_name = 'ims_correlation_results.csv'\n",
    "pron_save_name = 'pronostia_correlation_results.csv'\n",
    "\n",
    "# test adding the 0 or 1 for whether standard or Weibull loss functions\n",
    "dfi = pd.read_csv(ims_folder / ims_save_name)\n",
    "dfp = pd.read_csv(pron_folder / pron_save_name)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10,12),)\n",
    "sns.set(font_scale=1.1, style=\"whitegrid\")\n",
    "\n",
    "title_list = [r\"$\\bf{(a)}$\"+\" IMS Correlation of Loss Functions with Test $R^2$\", \n",
    "              r\"$\\bf{(b)}$\"+\" PRONOSTIA Correlation of Loss Functions with Test $R^2$\"]\n",
    "\n",
    "df_list = [dfi, dfp]\n",
    "\n",
    "for ax, df, title in zip(axes.flat, df_list, title_list):\n",
    "\n",
    "    df = df.dropna(axis=0)\n",
    "    \n",
    "    ax = sns.barplot(x='loss_func',y='corr',ax=ax, palette='rocket', data=df)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90,)\n",
    "\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    # ax.axes.get_yaxis().set_visible(False) # hide y-axis\n",
    "    ax.grid(alpha=0.7, linewidth=1, axis=\"y\")\n",
    "    ax.xaxis.set_label_text(\"\",  size='large', weight='semibold')\n",
    "    ax.yaxis.set_label_text(\"\", size='large', weight='semibold')\n",
    "    ax.set_yticks([0])\n",
    "    ax.set_yticklabels([], alpha=0)\n",
    "    ax.set_xticklabels([], alpha=0)\n",
    "    # plt.rc('xtick', labelsize='medium') \n",
    "    # plt.rc('font', weight='semibold') \n",
    "\n",
    "    for i, p in enumerate(ax.patches):\n",
    "        # help from https://stackoverflow.com/a/56780852/9214620\n",
    "        space = np.absolute(df['corr'].max()*0.1)\n",
    "\n",
    "        value = p.get_height()\n",
    "        if value >=0:\n",
    "            _y = p.get_y() + p.get_height() + float(space)\n",
    "            _x = p.get_x() + p.get_width() / 2\n",
    "            ax.text(_x, _y, f\"{value:.2f}\", ha=\"center\", va=\"center\", weight=\"semibold\", size=14)\n",
    "    #         ax.text(_x, -space, df_r['loss_func'][i], ha=\"center\", va=\"top\", weight=\"normal\",multialignment='right', size=12, \n",
    "    #                 rotation='vertical'\n",
    "    #                )\n",
    "            ax.text(_x+0.2, -0.01, df['loss_func'][i], ha=\"right\", va=\"top\", weight=\"normal\",multialignment='right', size=14, \n",
    "                    rotation=65\n",
    "                   )\n",
    "\n",
    "        else:\n",
    "            _y = p.get_y() + p.get_height() - float(space)\n",
    "            _x = p.get_x() + p.get_width() / 2\n",
    "            ax.text(_x, _y, f\"{value:.2f}\", ha=\"center\", va=\"center\", weight=\"semibold\", size=14)\n",
    "            ax.text(_x-0.2, 0.01, df['loss_func'][i], ha=\"left\", va=\"bottom\", weight=\"normal\",multialignment='left', size=14, \n",
    "                    rotation=65\n",
    "                   )\n",
    "\n",
    "\n",
    "    plt.rcParams['axes.titlepad'] = 20\n",
    "    ax.set_title(title, loc=\"left\", size=14)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "plt.savefig('correlations.svg',dpi=300, format='svg', bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Best Model and Check Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = Path('/home/tim/Documents/bearing/data/processed/IMS')\n",
    "\n",
    "(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    x_train_2,\n",
    "    y_train_2,\n",
    "    x_train_3,\n",
    "    y_train_3,\n",
    ") = load_train_test(folder_path)\n",
    "\n",
    "\n",
    "# load beta, eta for Weibull CDF\n",
    "with h5py.File(folder_path / \"eta_beta_r.hdf5\", \"r\") as f:\n",
    "    eta_beta_r = f[\"eta_beta_r\"][:]\n",
    "\n",
    "ETA = eta_beta_r[0]\n",
    "BETA = eta_beta_r[1]\n",
    "\n",
    "y_train_days = torch.reshape(y_train[:, 0], (-1, 1))\n",
    "y_val_days = torch.reshape(y_val[:, 0], (-1, 1))\n",
    "y_test_days = torch.reshape(y_test[:, 0], (-1, 1))\n",
    "\n",
    "y_train_days_2 = torch.reshape(y_train_2[:, 0], (-1, 1))\n",
    "y_train_days_3 = torch.reshape(y_train_3[:, 0], (-1, 1))\n",
    "\n",
    "\n",
    "y_train = torch.reshape(y_train[:, 1], (-1, 1))\n",
    "y_val = torch.reshape(y_val[:, 1], (-1, 1))\n",
    "y_test = torch.reshape(y_test[:, 1], (-1, 1))\n",
    "\n",
    "y_train_2 = torch.reshape(y_train_2[:, 1], (-1, 1))\n",
    "y_train_3 = torch.reshape(y_train_3[:, 1], (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(f\"{df['lambda_mod'][model_index]:1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframes\n",
    "ims_folder = Path('/home/tim/Documents/bearing/notebooks/results_complete/2021.04.05_results_ims')\n",
    "pron_folder = Path('/home/tim/Documents/bearing/notebooks/results_complete/2021.04.07_results_femto')\n",
    "\n",
    "ims_save_name = 'best_results_ims.csv'\n",
    "pron_save_name = 'best_results_pronostia.csv'\n",
    "\n",
    "# load best results csv so that we can get the best model\n",
    "df_ims = pd.read_csv(ims_folder / ims_save_name)\n",
    "# should already be sorted, but make sure\n",
    "df_ims = df_ims.sort_values(by='r2_test',ascending=False)\n",
    "\n",
    "df_pron = pd.read_csv(pron_folder / pron_save_name)\n",
    "df_pron = df_pron.sort_values(by='r2_test',ascending=False)\n",
    "\n",
    "\n",
    "model_index = 0\n",
    "\n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "for df, unit_data, date_mod in zip([df_ims, df_pron], ['days', 'hours'], [1, 24]):\n",
    "    dfm = pd.DataFrame()\n",
    "    dfm['Loss Function'] = [df['loss_func'][model_index]]\n",
    "    dfm['Loss Function'] = dfm[['Loss Function']].apply(change_loss_func_name, axis=1)\n",
    "    dfm['Layers'] = [df['n_layers'][model_index]]\n",
    "    dfm['Units per Layer'] = [df['n_units'][model_index]]\n",
    "    dfm['Drop Prob.'] = [df['prob_drop'][model_index]]\n",
    "    dfm['λ'] = [f\"{df['lambda_mod'][model_index]:.2f}\"]\n",
    "    dfm['β'] = [df['beta'][model_index]]\n",
    "    dfm['η'] = [f\"{df['eta'][model_index]*date_mod:.1f} {unit_data}\"]\n",
    "    \n",
    "    \n",
    "    df_all = df_all.append(dfm)\n",
    "\n",
    "\n",
    "df_all = df_all.T\n",
    "df_all.columns = ['IMS', 'PRONOSTIA']\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_layers'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best results csv so that we can get the best model\n",
    "df_best = pd.read_csv('best_results_ims.csv')\n",
    "\n",
    "# should already be sorted, but make sure\n",
    "df_best = df_best.sort_values(by='r2_test',ascending=False)\n",
    "\n",
    "# seclect top performing model and get the name it was saved under\n",
    "model_name = df_best['model_checkpoint_name'][0]\n",
    "print(model_name)\n",
    "\n",
    "model_path = Path('/home/tim/Documents/bearing/notebooks/results_complete/2021.04.05_results_ims/checkpoints') / model_name\n",
    "\n",
    "# select device to run neural net on\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Running on GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on CPU\")\n",
    "    \n",
    "criterion_mae = nn.L1Loss()\n",
    "criterion_rmse = RMSELoss()\n",
    "criterion_rmsle = RMSLELoss()\n",
    "\n",
    "net = torch.load(model_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "# from matplotlib import rc\n",
    "# rc('text', usetex=True)\n",
    "\n",
    "\n",
    "def calc_r2_avg(y_hats, y_val, index_sorted, window_size):\n",
    "    y_hats_rolling_avg = np.convolve(np.array(y_hats[index_sorted]).reshape(-1), np.ones(window_size), 'valid') / window_size\n",
    "    r2_val_avg = r2_score(np.array(y_val)[index_sorted][window_size-1:], y_hats_rolling_avg)\n",
    "    return r2_val_avg, y_hats_rolling_avg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### CREATE FIGURE #####\n",
    "# color blind colors, from https://bit.ly/3qJ6LYL\n",
    "# [#d73027, #fc8d59, #fee090, #4575b4]\n",
    "# [redish, orangeish, yellowish, blueish]\n",
    "\n",
    "sns.set(font_scale=1.0, style=\"whitegrid\", )\n",
    "\n",
    "# establish subplot axes\n",
    "# helpful matplotlib guide: \n",
    "# https://matplotlib.org/2.0.2/users/gridspec.html\n",
    "fig = plt.figure(figsize=(14, 7), dpi=150) \n",
    "gs = gridspec.GridSpec(3, 3)\n",
    "\n",
    "# ax1 = plt.subplot(gs[:, :2])\n",
    "# ax2 = plt.subplot(gs[0, 2])\n",
    "# ax3 = plt.subplot(gs[1, 2])\n",
    "# ax4 = plt.subplot(gs[2, 2])\n",
    "\n",
    "ax1 = plt.subplot(gs[:, 1:])\n",
    "ax2 = plt.subplot(gs[2, 0])\n",
    "ax3 = plt.subplot(gs[0, 0])\n",
    "ax4 = plt.subplot(gs[1, 0])\n",
    "gs.update(wspace = 0.4, hspace = 0.5)\n",
    "\n",
    "\n",
    "## General Formatting ##\n",
    "# create list of axis elements\n",
    "axes_list = [ax1, ax2, ax3, ax4]\n",
    "\n",
    "for ax in axes_list:\n",
    "    ax.grid(b=None)\n",
    "\n",
    "\n",
    "###### TEST DATA #####\n",
    "net.eval()\n",
    "\n",
    "y_hats = test(net, x_test, device, 100)\n",
    "index_sorted = np.array(np.argsort(y_test, 0).reshape(-1))\n",
    "\n",
    "# build rolling average\n",
    "window_size = 12 # 2 hour rolling avg\n",
    "r2_test_avg, y_hats_rolling_avg = calc_r2_avg(y_hats, y_test, index_sorted, window_size)\n",
    "\n",
    "loss_rmse_test = criterion_rmse(y_hats, y_test)\n",
    "r2_test = r2_score(y_test, y_hats)\n",
    "    \n",
    "ax1.plot(np.array(y_test)[index_sorted]*100, label=\"True Life Percentage\", alpha=1, color='#4575b4',linewidth=2, zorder=0)\n",
    "ax1.scatter(\n",
    "    np.arange(0, len(y_hats), 1),\n",
    "    y_hats[index_sorted]*100,\n",
    "    label=\"Predicted Life Percentage\",\n",
    "    alpha=0.4,\n",
    "    c=\"grey\",\n",
    "    s=9, edgecolors='none',\n",
    ")\n",
    "\n",
    "ax1.plot(np.arange(0, len(y_hats), 1)[window_size-1:], y_hats_rolling_avg*100, \n",
    "        color='#d73027', alpha=1, label=f'{int(window_size/6)}hr Rolling Avg', linewidth=2, )\n",
    "\n",
    "print_text = f\"RMSE = {loss_rmse_test:.3f}\\n$R^2$ = {r2_test:.3f}\"\n",
    "\n",
    "x_min, x_max = ax1.get_xlim()\n",
    "y_min, y_max = ax1.get_ylim()\n",
    "\n",
    "ax1.text(\n",
    "    (x_max - x_min) * 0.05 + x_min,\n",
    "    y_max - (y_max - y_min) * 0.05,\n",
    "    print_text,\n",
    "    fontsize=12,\n",
    "    fontweight=\"normal\",\n",
    "    verticalalignment=\"top\",\n",
    "    horizontalalignment=\"left\",\n",
    "    bbox={\"facecolor\": \"gray\", \"alpha\": 0.0, \"pad\": 6},\n",
    ")\n",
    "\n",
    "index_new = np.arange(0,len(y_hats),int(len(y_hats)/5)-1)\n",
    "\n",
    "y_days_temp = np.array(y_test_days)\n",
    "y_days_temp = np.reshape(y_days_temp, np.shape(y_days_temp)[0])[index_sorted]\n",
    "\n",
    "labels_new = [f'{i:.1f}' for i in y_days_temp[index_new]]\n",
    "# change first value to '0'\n",
    "labels_new[0] = '0'\n",
    "\n",
    "ax1.set_xticks(index_new)\n",
    "ax1.set_xticklabels(labels_new)\n",
    "ax1.set_xlabel(\"Days\")\n",
    "ax1.set_ylabel(\"Life Percentage\")\n",
    "ax1.legend(loc='lower right')\n",
    "\n",
    "\n",
    "plt.rcParams['axes.titlepad'] =5\n",
    "ax1.set_title(r\"$\\bf{(d)}$\"+\" Test Results (run 1, bearing 4)\", loc=\"left\")\n",
    "\n",
    "# secondary axis title list\n",
    "ax_title_list = [r\"$\\bf{(c)}$\"+\" Val Results (run 1, bearing 3)\", \n",
    "                 r\"$\\bf{(a)}$\"+\" Train Results (run 2, bearing 1)\", \n",
    "                 r\"$\\bf{(b)}$\"+\" Train Results (run 3, bearing 3)\"]\n",
    "\n",
    "\n",
    "# secondary axis counter\n",
    "counter = 0\n",
    "for ax, y_temp, x_temp, y_days, ax_title in zip([ax2,ax3,ax4],[y_val, y_train_2, y_train_3],\n",
    "                            [x_val, x_train_2, x_train_3], [y_val_days, y_train_days_2, y_train_days_3], ax_title_list):\n",
    "    \n",
    "    y_hats = test(net, x_temp, device, 100)\n",
    "    index_sorted = np.array(np.argsort(y_temp, 0).reshape(-1))\n",
    "\n",
    "    # build rolling average\n",
    "    window_size = 12 # 2 hour rolling avg\n",
    "    r2_test_avg, y_hats_rolling_avg = calc_r2_avg(y_hats, y_temp, index_sorted, window_size)\n",
    "\n",
    "    loss_rmse_test = criterion_rmse(y_hats, y_temp)\n",
    "    r2_test = r2_score(y_temp, y_hats)\n",
    "\n",
    "    ax.plot(np.array(y_temp)[index_sorted]*100, label=\"True Life Percentage\", alpha=1, color='#4575b4',linewidth=1, zorder=0)\n",
    "    ax.scatter(\n",
    "        np.arange(0, len(y_hats), 1),\n",
    "        y_hats[index_sorted]*100,\n",
    "        label=\"Predicted Life Percentage\",\n",
    "        alpha=0.4,\n",
    "        c=\"grey\", edgecolors='none',\n",
    "        s=2,\n",
    "    )\n",
    "\n",
    "    ax.plot(np.arange(0, len(y_hats), 1)[window_size-1:], y_hats_rolling_avg*100, \n",
    "            color='#d73027', alpha=1, label=f'{int(window_size/6)}hr Rolling Avg', linewidth=0.5)\n",
    "\n",
    "    print_text = f\"RMSE = {loss_rmse_test:.3f}\\n$R^2$ = {r2_test:.3f}\"\n",
    "\n",
    "    x_min, x_max = ax.get_xlim()\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "\n",
    "    ax.text(\n",
    "        (x_max - x_min) * 0.03 + x_min,\n",
    "        y_max - (y_max - y_min) * 0.05,\n",
    "        print_text,\n",
    "        fontsize=9,\n",
    "        fontweight=\"normal\",\n",
    "        verticalalignment=\"top\",\n",
    "        horizontalalignment=\"left\",\n",
    "        bbox={\"facecolor\": \"gray\", \"alpha\": 0.0, \"pad\": 6},\n",
    "    )\n",
    "\n",
    "    index_new = np.arange(0,len(y_hats),int(len(y_hats)/3)-1)\n",
    "\n",
    "    y_days_temp = np.array(y_days)\n",
    "    y_days_temp = np.reshape(y_days_temp, np.shape(y_days_temp)[0])[index_sorted]\n",
    "\n",
    "    labels_new = [f'{i:.1f}' for i in y_days_temp[index_new]]\n",
    "    # change first value to '0'\n",
    "    labels_new[0] = '0'\n",
    "\n",
    "    ax.set_xticks(index_new)\n",
    "    ax.set_xticklabels(labels_new,)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_title(ax_title, loc=\"left\")\n",
    "    \n",
    "    if counter == 0:\n",
    "        ax.set_xlabel(\"Days\")\n",
    "    \n",
    "    counter += 1\n",
    "    \n",
    "# plt.savefig('ims_result_plot.png',dpi=150, bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### CALCULATE METRICS ON TEST DATA #####\n",
    "net.eval()\n",
    "\n",
    "y_hats = test(net, x_test, device, 100)\n",
    "index_sorted = np.array(np.argsort(y_test, 0).reshape(-1))\n",
    "\n",
    "def calc_r2_avg(y_hats, y_val, index_sorted, window_size):\n",
    "    y_hats_rolling_avg = np.convolve(np.array(y_hats[index_sorted]).reshape(-1), np.ones(window_size), 'valid') / window_size\n",
    "    r2_val_avg = r2_score(np.array(y_val)[index_sorted][window_size-1:], y_hats_rolling_avg)\n",
    "    return r2_val_avg, y_hats_rolling_avg\n",
    "\n",
    "# build rolling average\n",
    "window_size = 12 # 2 hour rolling avg\n",
    "r2_test_avg, y_hats_rolling_avg = calc_r2_avg(y_hats, y_test, index_sorted, window_size)\n",
    "\n",
    "loss_rmse_test = criterion_rmse(y_hats, y_test)\n",
    "# loss_mae_test = criterion_mae(y_hats, y_test)\n",
    "# loss_rmsle_test = criterion_rmsle(y_hats, y_test)\n",
    "r2_test = r2_score(y_test, y_hats)\n",
    "\n",
    "\n",
    "###### CREATE FIGURE #####\n",
    "# color blind colors, from https://bit.ly/3qJ6LYL\n",
    "# [#d73027, #fc8d59, #fee090, #4575b4]\n",
    "# [redish, orangeish, yellowish, blueish]\n",
    "# ['mse', 'physics_loss', approx_constraint_loss, 'combined']\n",
    "sns.set(font_scale=1.0, style=\"whitegrid\")\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5), constrained_layout=True,)\n",
    "\n",
    "ax.plot(np.array(y_test)[index_sorted]*100, label=\"True Life Percentage\", alpha=1, color='#4575b4',linewidth=2, zorder=0)\n",
    "ax.scatter(\n",
    "    np.arange(0, len(y_hats), 1),\n",
    "    y_hats[index_sorted]*100,\n",
    "    label=\"Predicted Life Percentage\",\n",
    "    alpha=0.4,\n",
    "    c=\"#d73027\",\n",
    "    s=2,\n",
    ")\n",
    "\n",
    "ax.plot(np.arange(0, len(y_hats), 1)[window_size-1:], y_hats_rolling_avg*100, \n",
    "        color='#d73027', alpha=1, label=f'{int(window_size/6)}hr Rolling Avg', linewidth=2)\n",
    "\n",
    "print_text = f\"RMSE = {loss_rmse_test:.3f}\\n$R^2$ = {r2_test:.3f}\"\n",
    "\n",
    "x_min, x_max = ax.get_xlim()\n",
    "y_min, y_max = ax.get_ylim()\n",
    "\n",
    "ax.text(\n",
    "    (x_max - x_min) * 0.95 + x_min,\n",
    "    y_max - (y_max - y_min) * 0.90,\n",
    "    print_text,\n",
    "    fontsize=12,\n",
    "    fontweight=\"normal\",\n",
    "    verticalalignment=\"center\",\n",
    "    horizontalalignment=\"right\",\n",
    "    bbox={\"facecolor\": \"gray\", \"alpha\": 0.0, \"pad\": 6},\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ax.set_yticks(np.arange(1.5,20.5,2))\n",
    "\n",
    "# labels = ax.get_xticklabels()\n",
    "# loc = ax.get_xticks()\n",
    "plt.grid(b=None)\n",
    "\n",
    "\n",
    "\n",
    "index_new = np.arange(0,len(y_hats),int(len(y_hats)/5))\n",
    "\n",
    "y_days_temp = np.array(y_test_days)\n",
    "y_days_temp = np.reshape(y_days_temp, np.shape(y_days_temp)[0])[index_sorted]\n",
    "\n",
    "labels_new = [f'{i:.1f}' for i in y_days_temp[index_new]]\n",
    "# change first value to '0'\n",
    "labels_new[0] = '0'\n",
    "\n",
    "ax.set_xticks(index_new)\n",
    "ax.set_xticklabels(labels_new)\n",
    "ax.set_xlabel(\"Days\")\n",
    "ax.set_ylabel(\"Life Percentage\")\n",
    "ax.legend()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "# from matplotlib import rc\n",
    "# rc('text', usetex=True)\n",
    "\n",
    "\n",
    "def calc_r2_avg(y_hats, y_val, index_sorted, window_size):\n",
    "    y_hats_rolling_avg = np.convolve(np.array(y_hats[index_sorted]).reshape(-1), np.ones(window_size), 'valid') / window_size\n",
    "    r2_val_avg = r2_score(np.array(y_val)[index_sorted][window_size-1:], y_hats_rolling_avg)\n",
    "    return r2_val_avg, y_hats_rolling_avg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### CREATE FIGURE #####\n",
    "# color blind colors, from https://bit.ly/3qJ6LYL\n",
    "# [#d73027, #fc8d59, #fee090, #4575b4]\n",
    "# [redish, orangeish, yellowish, blueish]\n",
    "\n",
    "sns.set(font_scale=1.0, style=\"whitegrid\", )\n",
    "\n",
    "# establish subplot axes\n",
    "# helpful matplotlib guide: \n",
    "# https://matplotlib.org/2.0.2/users/gridspec.html\n",
    "fig = plt.figure(figsize=(11, 8), dpi=150) \n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "# ax1 = plt.subplot(gs[:, :2])\n",
    "# ax2 = plt.subplot(gs[0, 2])\n",
    "# ax3 = plt.subplot(gs[1, 2])\n",
    "# ax4 = plt.subplot(gs[2, 2])\n",
    "\n",
    "ax1 = plt.subplot(gs[0, 0])\n",
    "ax2 = plt.subplot(gs[0, 1])\n",
    "ax3 = plt.subplot(gs[1, 0])\n",
    "ax4 = plt.subplot(gs[1, 1])\n",
    "gs.update(wspace = 0.2, hspace = 0.4)\n",
    "\n",
    "\n",
    "## General Formatting ##\n",
    "# create list of axis elements\n",
    "axes_list = [ax1, ax2, ax3, ax4]\n",
    "\n",
    "for ax in axes_list:\n",
    "    ax.grid(b=None)\n",
    "\n",
    "###### TEST DATA #####\n",
    "net.eval()\n",
    "\n",
    "plt.rcParams['axes.titlepad'] = 7\n",
    "\n",
    "# secondary axis title list\n",
    "ax_title_list = [ \n",
    "                 r\"$\\bf{(a)}$\"+\" Train Results (run 2, bearing 1)\", \n",
    "                 r\"$\\bf{(b)}$\"+\" Train Results (run 3, bearing 3)\",\n",
    "                    r\"$\\bf{(c)}$\"+\" Val Results (run 1, bearing 3)\",\n",
    "                r\"$\\bf{(d)}$\"+\" Test Results (run 1, bearing 4)\"]\n",
    "\n",
    "\n",
    "# secondary axis counter\n",
    "counter = 0\n",
    "for ax, y_temp, x_temp, y_days, ax_title in zip([ax1, ax2,ax3,ax4],[y_train_2, y_train_3, y_val, y_test],\n",
    "                            [x_train_2, x_train_3, x_val, x_test], [y_train_days_2, y_train_days_3, y_val_days, y_test_days], ax_title_list):\n",
    "    \n",
    "    y_hats = test(net, x_temp, device, 100)\n",
    "    index_sorted = np.array(np.argsort(y_temp, 0).reshape(-1))\n",
    "\n",
    "    # build rolling average\n",
    "    window_size = 12 # 2 hour rolling avg\n",
    "    r2_test_avg, y_hats_rolling_avg = calc_r2_avg(y_hats, y_temp, index_sorted, window_size)\n",
    "\n",
    "    loss_rmse_test = criterion_rmse(y_hats, y_temp)\n",
    "    r2_test = r2_score(y_temp, y_hats)\n",
    "\n",
    "    ax.plot(np.array(y_temp)[index_sorted]*100, label=\"True Life Percentage\", alpha=1, color='#4575b4',linewidth=1, zorder=0)\n",
    "    ax.scatter(\n",
    "        np.arange(0, len(y_hats), 1),\n",
    "        y_hats[index_sorted]*100,\n",
    "        label=\"Predicted Life Percentage\",\n",
    "        alpha=0.4,\n",
    "        c=\"grey\", edgecolors='none',\n",
    "        s=2,\n",
    "    )\n",
    "\n",
    "    ax.plot(np.arange(0, len(y_hats), 1)[window_size-1:], y_hats_rolling_avg*100, \n",
    "            color='#d73027', alpha=1, label=f'{int(window_size/6)}hr Rolling Avg', linewidth=0.5)\n",
    "\n",
    "    print_text = f\"RMSE = {loss_rmse_test:.3f}\\n$R^2$ = {r2_test:.3f}\"\n",
    "\n",
    "    x_min, x_max = ax.get_xlim()\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "\n",
    "    ax.text(\n",
    "        (x_max - x_min) * 0.03 + x_min,\n",
    "        y_max - (y_max - y_min) * 0.05,\n",
    "        print_text,\n",
    "        fontsize=9,\n",
    "        fontweight=\"normal\",\n",
    "        verticalalignment=\"top\",\n",
    "        horizontalalignment=\"left\",\n",
    "        bbox={\"facecolor\": \"gray\", \"alpha\": 0.0, \"pad\": 6},\n",
    "    )\n",
    "\n",
    "    index_new = np.arange(0,len(y_hats),int(len(y_hats)/3)-1)\n",
    "\n",
    "    y_days_temp = np.array(y_days)\n",
    "    y_days_temp = np.reshape(y_days_temp, np.shape(y_days_temp)[0])[index_sorted]\n",
    "\n",
    "    labels_new = [f'{i:.1f}' for i in y_days_temp[index_new]]\n",
    "    # change first value to '0'\n",
    "    labels_new[0] = '0'\n",
    "\n",
    "    ax.set_xticks(index_new)\n",
    "    ax.set_xticklabels(labels_new,)\n",
    "#     ax.set_yticklabels([])\n",
    "    ax.set_title(ax_title, loc=\"left\")\n",
    "    \n",
    "    if counter == 0:\n",
    "        ax.set_xlabel(\"Runtime (days)\")\n",
    "        ax.set_ylabel(\"Life Percentage\")\n",
    "        ax.legend(loc='lower right', fontsize=10)\n",
    "        \n",
    "    if counter !=0:\n",
    "        ax.set_yticklabels([])\n",
    "    \n",
    "    counter += 1\n",
    "    \n",
    "# plt.savefig('ims_result_plot.svg',dpi=300, format='svg', bbox_inches = \"tight\")\n",
    "plt.savefig('ims_result_plot.png',dpi=300, format='png', bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_list\n",
    "y_list = [y_train_2, y_train_3, y_val, y_test]\n",
    "\n",
    "# x_list\n",
    "x_list = [x_train_2, x_train_3, x_val, x_test]\n",
    "\n",
    "# y_days_list\n",
    "y_days_list = [y_train_days_2, y_train_days_3, y_val_days, y_test_days]\n",
    "\n",
    "\n",
    "val_max_list = [0.3, 0.4, 0.3, \n",
    "                0.5, 0.4, 0.4, \n",
    "                0.6, 0.8, 0.93]\n",
    "\n",
    "color_scheme='inferno'\n",
    "\n",
    "# vmax_x = 0\n",
    "# for x in x_list:\n",
    "#     print(torch.max(x).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "# from matplotlib import rc\n",
    "# rc('text', usetex=True)\n",
    "\n",
    "\n",
    "###### CREATE FIGURE #####\n",
    "\n",
    "sns.set(font_scale=1.0, style=\"whitegrid\", )\n",
    "\n",
    "# establish subplot axes\n",
    "# helpful matplotlib guide: \n",
    "# https://matplotlib.org/2.0.2/users/gridspec.html\n",
    "fig = plt.figure(figsize=(11, 8), dpi=150) \n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "ax1 = plt.subplot(gs[0, 0])\n",
    "ax2 = plt.subplot(gs[0, 1])\n",
    "ax3 = plt.subplot(gs[1, 0])\n",
    "ax4 = plt.subplot(gs[1, 1])\n",
    "gs.update(wspace = 0.2, hspace = 0.3)\n",
    "\n",
    "\n",
    "## General Formatting ##\n",
    "# create list of axis elements\n",
    "axes_list = [ax1, ax2, ax3, ax4]\n",
    "\n",
    "for ax in axes_list:\n",
    "    ax.grid(b=None)\n",
    "\n",
    "###### TEST DATA #####\n",
    "net.eval()\n",
    "\n",
    "plt.rcParams['axes.titlepad'] = 7\n",
    "\n",
    "# secondary axis title list\n",
    "ax_title_list = [ \n",
    "                \"(a)\"+\" Train Data (run 2, bearing 1)\", \n",
    "                 \"(b)\"+\" Train Data (run 3, bearing 3)\",\n",
    "                    \"(c)\"+\" Val Data (run 1, bearing 3)\",\n",
    "                \"(d)\"+\" Test Data (run 1, bearing 4)\"]\n",
    "\n",
    "\n",
    "counter = 0\n",
    "for ax, ax_title, y_temp, x_temp, y_days in zip(axes_list, ax_title_list, y_list, x_list, y_days_list):\n",
    "\n",
    "    index_sorted = np.array(np.argsort(y_temp, 0).reshape(-1))\n",
    "    \n",
    "#     vmax_val = np.max(np.max(df_spec))*val_max_mod\n",
    "    # create the time list that we'll use to label x-axis of spectrogram\n",
    "#     time_list = []\n",
    "\n",
    "#     for k in labels_dict:\n",
    "#         time_list.append(labels_dict[k][-1])\n",
    "\n",
    "    time_array = np.sort(y_days[:,-1])\n",
    "    \n",
    "    index_new = np.arange(0,len(time_array),int(len(time_array)/3)-1)\n",
    "    \n",
    "    labels_new = [f'{i:.1f}' for i in time_array[index_new]]\n",
    "    # change first value to '0'\n",
    "    labels_new[0] = '0'\n",
    "    \n",
    "    ax.pcolormesh(x_temp[index_sorted].T, \n",
    "                cmap=color_scheme,\n",
    "                 vmax=0.5,\n",
    "              )\n",
    "\n",
    "    ax.set_xticks(index_new)\n",
    "    ax.set_xticklabels(labels_new,)\n",
    "    \n",
    "    ax.text(0.02, 0.97, ax_title,\n",
    "        verticalalignment='top', horizontalalignment='left',\n",
    "        transform=ax.transAxes,\n",
    "        color='white', fontsize=12)\n",
    "    \n",
    "    if counter == 0:\n",
    "        ax.set_xticks(index_new)\n",
    "        ax.set_xticklabels(labels_new,)\n",
    "        ax.set_yticks(np.arange(3.5,20.5,4))\n",
    "        ax.set_yticklabels(list(np.arange(4,21,4)))\n",
    "        ax.set_ylabel('Frequency Bin')\n",
    "        ax.set_xlabel('Runtime (days)')\n",
    "    else:\n",
    "        ax.set_yticklabels([])\n",
    "        \n",
    "    if counter !=0:\n",
    "        ax.set_yticklabels([])\n",
    "    \n",
    "    \n",
    "    \n",
    "    counter += 1\n",
    "    \n",
    "\n",
    "    \n",
    "sns.despine(left=True, bottom=True, right=True)\n",
    "plt.savefig('spectrogram_processed_data_IMS.png',format='png', dpi=300, bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
